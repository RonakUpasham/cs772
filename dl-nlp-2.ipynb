{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7967725,"sourceType":"datasetVersion","datasetId":4688011},{"sourceId":8142610,"sourceType":"datasetVersion","datasetId":4814433},{"sourceId":8308318,"sourceType":"datasetVersion","datasetId":4935077},{"sourceId":8308380,"sourceType":"datasetVersion","datasetId":4935127}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer\n\nmodel = T5ForConditionalGeneration.from_pretrained('t5-large')\ntokenizer = T5Tokenizer.from_pretrained('t5-large')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-04T07:14:23.854154Z","iopub.execute_input":"2024-05-04T07:14:23.854812Z","iopub.status.idle":"2024-05-04T07:14:27.254994Z","shell.execute_reply.started":"2024-05-04T07:14:23.854780Z","shell.execute_reply":"2024-05-04T07:14:27.254051Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LINK PREDICTION","metadata":{}},{"cell_type":"markdown","source":"# load the dataset","metadata":{}},{"cell_type":"code","source":"### IGNORE\nwith open('/kaggle/input/project-dataset/kb.txt', 'r') as file:\n    lines = file.readlines()\n    number_of_lines = len(lines)\nprint(number_of_lines)\n\npredict_head = []\npredict_tail = []\ninputs = []\ntargets = []\n\nfor line_index in range(len(lines)):\n    line_split = lines[line_index].strip().split('|')\n    subject = line_split[0]\n    predicate = line_split[1]\n    obj = line_split[2]\n    if(line_index < len(lines)/2):\n        predict_head_input = f\"? | {predicate} | {obj}\"\n        predict_head_output = subject\n        predict_head.append((predict_head_input, predict_head_output))\n        inputs.append(predict_head_input)\n        targets.append(predict_head_output)\n    else:\n        predict_tail_input = f\"{subject} | {predicate} | ?\"\n        predict_tail_output = obj\n        predict_tail.append((predict_tail_input, predict_tail_output))\n        inputs.append(predict_tail_input)\n        targets.append(predict_tail_output)\n        \n# print(len(predict_head[0]))\n# print(predict_tail[:10])\nprint(inputs[:10], targets[:10])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T17:00:22.200944Z","iopub.execute_input":"2024-05-03T17:00:22.201782Z","iopub.status.idle":"2024-05-03T17:00:28.160409Z","shell.execute_reply.started":"2024-05-03T17:00:22.201749Z","shell.execute_reply":"2024-05-03T17:00:28.159537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# split the data for training, validation & testing","metadata":{}},{"cell_type":"code","source":"### IGNORE\nfrom sklearn.model_selection import train_test_split\n\n# Split data into training and temporary data (temp will be split into validation and test)\ninput_train, input_temp, target_train, target_temp = train_test_split(\n    inputs, targets, test_size=0.2, random_state=42)  # 80% train, 20% temp\n\n# Split the temporary data into validation and test sets\ninput_val, input_test, target_val, target_test = train_test_split(\n    input_temp, target_temp, test_size=0.5, random_state=42)  # Split temp equally into val and test\n\n# Print the sizes of each dataset to verify the splits\nprint(f\"Training set size: {len(input_train)}\")\nprint(f\"Validation set size: {len(input_val)}\")\nprint(f\"Test set size: {len(input_test)}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T17:01:04.397383Z","iopub.execute_input":"2024-05-03T17:01:04.397737Z","iopub.status.idle":"2024-05-03T17:01:05.629917Z","shell.execute_reply.started":"2024-05-03T17:01:04.397708Z","shell.execute_reply":"2024-05-03T17:01:05.628972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### IGNORE\ndef save_dataset(file_path, inputs, targets):\n    with open(file_path, 'w', encoding='utf-8') as file:\n        for input_text, target_text in zip(inputs, targets):\n            file.write(f\"{input_text}\\t{target_text}\\n\")\n\n# Save datasets to separate files\nsave_dataset(\"/kaggle/working/train_dataset.txt\", input_train, target_train)\nsave_dataset(\"/kaggle/working/validation_dataset.txt\", input_val, target_val)\nsave_dataset(\"/kaggle/working/test_dataset.txt\", input_test, target_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T20:07:02.669720Z","iopub.execute_input":"2024-05-03T20:07:02.670087Z","iopub.status.idle":"2024-05-03T20:07:02.760122Z","shell.execute_reply.started":"2024-05-03T20:07:02.670062Z","shell.execute_reply":"2024-05-03T20:07:02.759311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### IGNORE\nimport pandas as pd\n\ndef save_dataset_pandas(file_path, inputs, targets):\n    df = pd.DataFrame({'input': inputs, 'target': targets})\n    df.to_csv(file_path, index=False, sep='\\t')\n\n# Save datasets to separate files\nsave_dataset_pandas(\"/kaggle/working/train_dataset.tsv\", input_train, target_train)\nsave_dataset_pandas(\"/kaggle/working/validation_dataset.tsv\", input_val, target_val)\nsave_dataset_pandas(\"/kaggle/working/test_dataset.tsv\", input_test, target_test)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dataset(file_path):\n    inputs = []\n    targets = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            input_text, target_text = line.strip().split('\\t')\n            inputs.append(input_text)\n            targets.append(target_text)\n    return inputs, targets\n\n# Load datasets\ntrain_inputs, train_targets = load_dataset(\"/kaggle/input/project-dataset-split/train_dataset.txt\")\n# validation_inputs, validation_targets = load_dataset(\"/kaggle/input/project-dataset-split/validation_dataset.txt\")\n# test_inputs, test_targets = load_dataset(\"/kaggle/input/project-dataset-split/test_dataset.txt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:18:57.883663Z","iopub.execute_input":"2024-05-04T07:18:57.884050Z","iopub.status.idle":"2024-05-04T07:18:58.022564Z","shell.execute_reply.started":"2024-05-04T07:18:57.884020Z","shell.execute_reply":"2024-05-04T07:18:58.021722Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#  tokenize data","metadata":{}},{"cell_type":"code","source":"def tokenize_data(inputs, targets, tokenizer, max_input_length=128, max_target_length=30):\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize training, validation, and test sets\ntrain_data = tokenize_data(train_inputs, train_targets, tokenizer)\n# val_data = tokenize_data(input_val, target_val, tokenizer)\n# test_data = tokenize_data(input_test, target_test, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T17:01:10.738367Z","iopub.execute_input":"2024-05-03T17:01:10.738917Z","iopub.status.idle":"2024-05-03T17:01:33.419508Z","shell.execute_reply.started":"2024-05-03T17:01:10.738887Z","shell.execute_reply":"2024-05-03T17:01:33.418483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(model_inputs))","metadata":{"execution":{"iopub.status.busy":"2024-05-03T14:05:21.979937Z","iopub.execute_input":"2024-05-03T14:05:21.980756Z","iopub.status.idle":"2024-05-03T14:05:22.363782Z","shell.execute_reply.started":"2024-05-03T14:05:21.980719Z","shell.execute_reply":"2024-05-03T14:05:22.362338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass T5Dataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\ntrain_dataset = T5Dataset(train_data)\n# val_dataset = T5Dataset(val_data)\n# test_dataset = T5Dataset(test_data)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n# val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n# test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T17:01:48.274543Z","iopub.execute_input":"2024-05-03T17:01:48.274918Z","iopub.status.idle":"2024-05-03T17:01:48.283202Z","shell.execute_reply.started":"2024-05-03T17:01:48.274891Z","shell.execute_reply":"2024-05-03T17:01:48.282101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_scheduler\nimport torch\n\noptimizer = AdamW(model.parameters(), lr=5e-4)\nnum_epochs = 2\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    print(f\"Epoch : {epoch}\")\n    i = 1\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        if i%10 == 0:\n            print(f\"{i} Training loss: {loss.item()}\")\n        i = i+1\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T17:01:51.634987Z","iopub.execute_input":"2024-05-03T17:01:51.635340Z","iopub.status.idle":"2024-05-03T19:15:32.955266Z","shell.execute_reply.started":"2024-05-03T17:01:51.635312Z","shell.execute_reply":"2024-05-03T19:15:32.954384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving only the model weights\nmodel_weights_path = \"/kaggle/working/model_weights.pth\"\ntorch.save(model.state_dict(), model_weights_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T19:18:39.677561Z","iopub.execute_input":"2024-05-03T19:18:39.677945Z","iopub.status.idle":"2024-05-03T19:18:44.365898Z","shell.execute_reply.started":"2024-05-03T19:18:39.677917Z","shell.execute_reply":"2024-05-03T19:18:44.364899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/T5-finetuned')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T19:19:59.938243Z","iopub.execute_input":"2024-05-03T19:19:59.938609Z","iopub.status.idle":"2024-05-03T19:20:04.714856Z","shell.execute_reply.started":"2024-05-03T19:19:59.938580Z","shell.execute_reply":"2024-05-03T19:20:04.713867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load the model","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Path to the saved weights\nmodel_weights_path = '/kaggle/input/t5-linkpred/model_weights.pth'\n\n# Load the weights into the model\nmodel.load_state_dict(torch.load(model_weights_path))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T05:13:33.493692Z","iopub.execute_input":"2024-05-04T05:13:33.494677Z","iopub.status.idle":"2024-05-04T05:14:00.193026Z","shell.execute_reply.started":"2024-05-04T05:13:33.494646Z","shell.execute_reply":"2024-05-04T05:14:00.192105Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"# load validation and test data","metadata":{}},{"cell_type":"code","source":"def load_dataset(file_path):\n    inputs = []\n    targets = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            input_text, target_text = line.strip().split('\\t')\n            inputs.append(input_text)\n            targets.append(target_text)\n    return inputs, targets\n\n# Load datasets\n# train_inputs, train_targets = load_dataset(\"train_dataset.txt\")\nvalidation_inputs, validation_targets = load_dataset(\"/kaggle/input/project-dataset-split/validation_dataset.txt\")\ntest_inputs, test_targets = load_dataset(\"/kaggle/input/project-dataset-split/test_dataset.txt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T05:16:22.697629Z","iopub.execute_input":"2024-05-04T05:16:22.698010Z","iopub.status.idle":"2024-05-04T05:16:22.746917Z","shell.execute_reply.started":"2024-05-04T05:16:22.697981Z","shell.execute_reply":"2024-05-04T05:16:22.746121Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(len(validation_inputs))\nprint(len(test_inputs))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T05:16:53.964020Z","iopub.execute_input":"2024-05-04T05:16:53.964808Z","iopub.status.idle":"2024-05-04T05:16:53.970262Z","shell.execute_reply.started":"2024-05-04T05:16:53.964747Z","shell.execute_reply":"2024-05-04T05:16:53.969334Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"13474\n13475\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_data(inputs, targets, tokenizer, max_input_length=128, max_target_length=30):\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize training, validation, and test sets\n# train_data = tokenize_data(input_train, target_train, tokenizer)\nval_data = tokenize_data(validation_inputs, validation_targets, tokenizer)\ntest_data = tokenize_data(test_inputs, test_targets, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T05:19:11.572502Z","iopub.execute_input":"2024-05-04T05:19:11.572926Z","iopub.status.idle":"2024-05-04T05:19:16.123156Z","shell.execute_reply.started":"2024-05-04T05:19:11.572891Z","shell.execute_reply":"2024-05-04T05:19:16.122361Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass T5Dataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n# train_dataset = T5Dataset(train_data)\nval_dataset = T5Dataset(val_data)\ntest_dataset = T5Dataset(test_data)\n\n# train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T05:19:51.518355Z","iopub.execute_input":"2024-05-04T05:19:51.519270Z","iopub.status.idle":"2024-05-04T05:19:51.526327Z","shell.execute_reply.started":"2024-05-04T05:19:51.519234Z","shell.execute_reply":"2024-05-04T05:19:51.525357Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T05:20:44.785215Z","iopub.execute_input":"2024-05-04T05:20:44.785876Z","iopub.status.idle":"2024-05-04T05:20:44.805394Z","shell.execute_reply.started":"2024-05-04T05:20:44.785843Z","shell.execute_reply":"2024-05-04T05:20:44.804198Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 1024)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 16)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 16)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\ndef evaluate_model(model, dataloader):\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model.to(device)\n    model.eval()  # Make sure model is in eval mode\n    total_loss = 0\n    predictions, true_labels = [], []\n\n    with torch.no_grad():  # Disable gradient calculation\n        for batch in dataloader:\n            # Move batch to the same device as the model\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n\n            # Collect the loss and calculate the average\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            # Decode predictions\n            predicted_ids = torch.argmax(outputs.logits, dim=-1)\n#             predicted_tokens = [tokenizer.decode(ids) for ids in predicted_ids]\n            predicted_tokens = [tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n            predictions.extend(predicted_tokens)\n            true_labels.extend([tokenizer.decode(ids, skip_special_tokens=True) for ids in batch['labels']])\n\n    # Calculate average loss\n    average_loss = total_loss / len(dataloader)\n    return predictions, true_labels, average_loss\n\n# Evaluate on validation and test datasets\nprint(\"----- Validation -----\")\nval_predictions, val_labels, val_loss = evaluate_model(model, val_dataloader)\nprint(f\"Validation loss: {val_loss}\")\nprint(\"----- Test -----\")\ntest_predictions, test_labels, test_loss = evaluate_model(model, test_dataloader)\nprint(f\"Test loss: {test_loss}\")\n\n# Here you can further calculate accuracy or other metrics based on predictions and true_labels\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T06:13:52.868215Z","iopub.execute_input":"2024-05-04T06:13:52.868585Z","iopub.status.idle":"2024-05-04T06:26:34.515923Z","shell.execute_reply.started":"2024-05-04T06:13:52.868555Z","shell.execute_reply":"2024-05-04T06:26:34.514996Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"----- Validation -----\nValidation loss: 0.3567079286900755\n----- Test -----\nTest loss: 0.3577617426300615\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_accuracy(predictions, true_labels):\n    correct_predictions = sum(1 for pred, true in zip(predictions, true_labels) if pred == true)\n    total_predictions = len(predictions)\n    accuracy = correct_predictions / total_predictions\n    print(correct_predictions)\n    return accuracy\n\n# Calculate accuracy\nval_accuracy = calculate_accuracy(val_predictions, val_labels)\ntest_accuracy = calculate_accuracy(test_predictions, test_labels)\n\nprint(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\nprint(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T06:33:27.350091Z","iopub.execute_input":"2024-05-04T06:33:27.350725Z","iopub.status.idle":"2024-05-04T06:33:27.361894Z","shell.execute_reply.started":"2024-05-04T06:33:27.350693Z","shell.execute_reply":"2024-05-04T06:33:27.360849Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"340\n363\nValidation Loss: 0.3567079286900755, Validation Accuracy: 0.025233783583197267\nTest Loss: 0.3577617426300615, Test Accuracy: 0.026938775510204082\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print some sample predictions and labels\n# for i in range(len(val_predictions)):\n#     if val\n#     print(f\"Prediction: {val_predictions[i]}, True Label: {val_labels[i]}\")\nfor pred, true in zip(val_predictions, val_labels):\n    if pred == true:\n        print(f\"Prediction: {pred}, True Label: {true}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T06:35:36.067594Z","iopub.execute_input":"2024-05-04T06:35:36.067976Z","iopub.status.idle":"2024-05-04T06:35:36.079711Z","shell.execute_reply.started":"2024-05-04T06:35:36.067945Z","shell.execute_reply":"2024-05-04T06:35:36.078447Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Prediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: John Wayne, True Label: John Wayne\nPrediction: 2007, True Label: 2007\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: 2007, True Label: 2007\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: John Wayne, True Label: John Wayne\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: French, True Label: French\nPrediction: good, True Label: good\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: John Ford, True Label: John Ford\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: The Last of the Mohicans, True Label: The Last of the Mohicans\nPrediction: 2007, True Label: 2007\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: 2007, True Label: 2007\nPrediction: French, True Label: French\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: John Ford, True Label: John Ford\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: John Wayne, True Label: John Wayne\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: bd-r, True Label: bd-r\nPrediction: French, True Label: French\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: French, True Label: French\nPrediction: bd-r, True Label: bd-r\nPrediction: 2007, True Label: 2007\nPrediction: 2007, True Label: 2007\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: Drama, True Label: Drama\nPrediction: good, True Label: good\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: The Last of the Mohicans, True Label: The Last of the Mohicans\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: 2007, True Label: 2007\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: John Wayne, True Label: John Wayne\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: John Wayne, True Label: John Wayne\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: good, True Label: good\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: good, True Label: good\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: good, True Label: good\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: 2007, True Label: 2007\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: good, True Label: good\nPrediction: Drama, True Label: Drama\nPrediction: bd-r, True Label: bd-r\nPrediction: French, True Label: French\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: good, True Label: good\nPrediction: bd-r, True Label: bd-r\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: Drama, True Label: Drama\nPrediction: French, True Label: French\nPrediction: French, True Label: French\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_predictions_labels(predictions, labels, file_name):\n    with open(file_name, 'w', encoding='utf-8') as file:\n        for pred, label in zip(predictions, labels):\n            file.write(f\"{pred}\\t{label}\\n\")\n\n# Assuming you have val_predictions, val_labels, test_predictions, and test_labels\nsave_predictions_labels(val_predictions, val_labels, \"/kaggle/working/validation_predictions_labels.txt\")\nsave_predictions_labels(test_predictions, test_labels, \"/kaggle/working/test_predictions_labels.txt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T06:47:00.094474Z","iopub.execute_input":"2024-05-04T06:47:00.094814Z","iopub.status.idle":"2024-05-04T06:47:00.116064Z","shell.execute_reply.started":"2024-05-04T06:47:00.094777Z","shell.execute_reply":"2024-05-04T06:47:00.115278Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# QUESTION ANWERING","metadata":{}},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer\n\nmodel = T5ForConditionalGeneration.from_pretrained('t5-large')\ntokenizer = T5Tokenizer.from_pretrained('t5-large')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T09:42:57.588391Z","iopub.execute_input":"2024-05-04T09:42:57.589101Z","iopub.status.idle":"2024-05-04T09:43:16.511176Z","shell.execute_reply.started":"2024-05-04T09:42:57.589068Z","shell.execute_reply":"2024-05-04T09:43:16.509833Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed1ab8c0ed244bdaa7d5286b8dd0e426"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa7304dfcd4426fb784ca3d3fff2cc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d70612fbe594c1f82559e675952b9f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16b0f8c8a95045e2b69b501aba8d1bc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c89e4b61213470cbba9fdcb5dffefa8"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# preprocess and load dataset","metadata":{}},{"cell_type":"code","source":"def preprocess_data(input_file, output_file):\n    with open(input_file, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    processed_lines = []\n\n    for line in lines:\n        question, answers = line.strip().split('\\t')\n        # Ensure the question ends with a question mark\n        question = question.strip()\n        if not question.endswith('?'):\n            question += '?'\n\n        # Split answers if there are multiple answers separated by '|'\n        answers = answers.split('|')\n        for answer in answers:\n            processed_line = f\"{question}\\t{answer.strip()}\\n\"\n            processed_lines.append(processed_line)\n\n    # Write the processed lines to the output file\n    with open(output_file, 'w', encoding='utf-8') as file:\n        file.writelines(processed_lines)\n\n# Define your input and output file paths\ninput_file_path = '/kaggle/input/metaqa/qa_train.txt'\noutput_file_path = '/kaggle/working/qa_train_pp.txt'\n\n# Call the function with the file paths\npreprocess_data(input_file_path, output_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T09:26:53.541857Z","iopub.execute_input":"2024-05-04T09:26:53.542228Z","iopub.status.idle":"2024-05-04T09:26:53.875716Z","shell.execute_reply.started":"2024-05-04T09:26:53.542199Z","shell.execute_reply":"2024-05-04T09:26:53.874957Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def load_dataset(file_path):\n    inputs = []\n    targets = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            input_text, target_text = line.strip().split('\\t')\n            inputs.append(input_text)\n            targets.append(target_text)\n    return inputs, targets\nqa_train_inputs, qa_train_targets = load_dataset(\"/kaggle/working/qa_train_pp.txt\")\nprint(qa_train_inputs[:10], qa_train_targets[:10])","metadata":{"execution":{"iopub.status.busy":"2024-05-04T09:31:45.048401Z","iopub.execute_input":"2024-05-04T09:31:45.048973Z","iopub.status.idle":"2024-05-04T09:31:45.239679Z","shell.execute_reply.started":"2024-05-04T09:31:45.048942Z","shell.execute_reply":"2024-05-04T09:31:45.238541Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['what movies are about [ginger rogers]?', 'what movies are about [ginger rogers]?', 'what movies are about [ginger rogers]?', 'which movies can be described by [moore]?', 'which movies can be described by [moore]?', 'what films can be described by [occupation]?', 'what films can be described by [occupation]?', 'which films are about [jacques tati]?', 'which films are about [jacques tati]?', 'which films are about [jacques tati]?'] ['Top Hat', 'Kitty Foyle', 'The Barkleys of Broadway', 'Fahrenheit 9/11', 'Far from Heaven', 'Red Dawn', 'The Teahouse of the August Moon', 'Mon Oncle', 'Playtime', 'Trafic']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# tokenize data","metadata":{}},{"cell_type":"code","source":"def tokenize_data(inputs, targets, tokenizer, max_input_length=128, max_target_length=50):\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize training, validation, and test sets\nqa_train_data = tokenize_data(qa_train_inputs, qa_train_targets, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T09:45:41.061525Z","iopub.execute_input":"2024-05-04T09:45:41.062383Z","iopub.status.idle":"2024-05-04T09:46:15.976867Z","shell.execute_reply.started":"2024-05-04T09:45:41.062349Z","shell.execute_reply":"2024-05-04T09:46:15.976050Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass T5Dataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\nqa_train_dataset = T5Dataset(qa_train_data)\n# val_dataset = T5Dataset(val_data)\n# test_dataset = T5Dataset(test_data)\n\nqa_train_dataloader = DataLoader(qa_train_dataset, batch_size=8, shuffle=True)\n# val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n# test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T09:49:29.683218Z","iopub.execute_input":"2024-05-04T09:49:29.683609Z","iopub.status.idle":"2024-05-04T09:49:29.691056Z","shell.execute_reply.started":"2024-05-04T09:49:29.683578Z","shell.execute_reply":"2024-05-04T09:49:29.690068Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# load finetuned T5","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Path to the saved weights\nmodel_weights_path = '/kaggle/input/t5-linkpred/model_weights.pth'\n\n# Load the weights into the model\nmodel.load_state_dict(torch.load(model_weights_path))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T09:50:58.730757Z","iopub.execute_input":"2024-05-04T09:50:58.731642Z","iopub.status.idle":"2024-05-04T09:51:24.711323Z","shell.execute_reply.started":"2024-05-04T09:50:58.731607Z","shell.execute_reply":"2024-05-04T09:51:24.710281Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AdamW, get_scheduler\nimport torch\n\noptimizer = AdamW(model.parameters(), lr=5e-4)\nnum_epochs = 1\nnum_training_steps = num_epochs * len(qa_train_dataloader)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    print(f\"Epoch : {epoch}\")\n    i = 1\n    for batch in qa_train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        if i%10 == 0:\n            print(f\"{i} Training loss: {loss.item()}\")\n        i = i+1\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T10:02:41.267087Z","iopub.execute_input":"2024-05-04T10:02:41.267964Z","iopub.status.idle":"2024-05-04T14:16:17.689008Z","shell.execute_reply.started":"2024-05-04T10:02:41.267929Z","shell.execute_reply":"2024-05-04T14:16:17.688180Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch : 0\n10 Training loss: 0.21799565851688385\n20 Training loss: 0.2503877580165863\n30 Training loss: 0.24944278597831726\n40 Training loss: 0.3157113194465637\n50 Training loss: 0.30038830637931824\n60 Training loss: 0.2885052561759949\n70 Training loss: 0.2732456922531128\n80 Training loss: 0.21573850512504578\n90 Training loss: 0.2787550687789917\n100 Training loss: 0.2711539566516876\n110 Training loss: 0.41609352827072144\n120 Training loss: 0.2582980990409851\n130 Training loss: 0.23949582874774933\n140 Training loss: 0.227314755320549\n150 Training loss: 0.2575443685054779\n160 Training loss: 0.2113952785730362\n170 Training loss: 0.17997941374778748\n180 Training loss: 0.30445680022239685\n190 Training loss: 0.26993030309677124\n200 Training loss: 0.2831249535083771\n210 Training loss: 0.28735119104385376\n220 Training loss: 0.35535141825675964\n230 Training loss: 0.30472004413604736\n240 Training loss: 0.20521700382232666\n250 Training loss: 0.30886223912239075\n260 Training loss: 0.3746112585067749\n270 Training loss: 0.28244778513908386\n280 Training loss: 0.2958490252494812\n290 Training loss: 0.23461349308490753\n300 Training loss: 0.35048210620880127\n310 Training loss: 0.3563717007637024\n320 Training loss: 0.27681484818458557\n330 Training loss: 0.27480387687683105\n340 Training loss: 0.23592334985733032\n350 Training loss: 0.2921053171157837\n360 Training loss: 0.27113068103790283\n370 Training loss: 0.2778397798538208\n380 Training loss: 0.3553040027618408\n390 Training loss: 0.22598397731781006\n400 Training loss: 0.18747106194496155\n410 Training loss: 0.32274574041366577\n420 Training loss: 0.24687404930591583\n430 Training loss: 0.24066612124443054\n440 Training loss: 0.2232387214899063\n450 Training loss: 0.24936643242835999\n460 Training loss: 0.3511582911014557\n470 Training loss: 0.3032529652118683\n480 Training loss: 0.25665560364723206\n490 Training loss: 0.2164677083492279\n500 Training loss: 0.36788976192474365\n510 Training loss: 0.2421361804008484\n520 Training loss: 0.23215225338935852\n530 Training loss: 0.20332935452461243\n540 Training loss: 0.3307841122150421\n550 Training loss: 0.37156304717063904\n560 Training loss: 0.2509163022041321\n570 Training loss: 0.29304438829421997\n580 Training loss: 0.2890944182872772\n590 Training loss: 0.24876201152801514\n600 Training loss: 0.217262864112854\n610 Training loss: 0.39913439750671387\n620 Training loss: 0.30328792333602905\n630 Training loss: 0.20362693071365356\n640 Training loss: 0.18754321336746216\n650 Training loss: 0.229413241147995\n660 Training loss: 0.29212549328804016\n670 Training loss: 0.3836219906806946\n680 Training loss: 0.2334291785955429\n690 Training loss: 0.27538803219795227\n700 Training loss: 0.3202548921108246\n710 Training loss: 0.3769361972808838\n720 Training loss: 0.21461014449596405\n730 Training loss: 0.27525368332862854\n740 Training loss: 0.3560037910938263\n750 Training loss: 0.2770082950592041\n760 Training loss: 0.30997368693351746\n770 Training loss: 0.28727567195892334\n780 Training loss: 0.30363407731056213\n790 Training loss: 0.26957762241363525\n800 Training loss: 0.32779261469841003\n810 Training loss: 0.39081406593322754\n820 Training loss: 0.16434051096439362\n830 Training loss: 0.3171691596508026\n840 Training loss: 0.2376749962568283\n850 Training loss: 0.2553301751613617\n860 Training loss: 0.19262275099754333\n870 Training loss: 0.2639446556568146\n880 Training loss: 0.20990172028541565\n890 Training loss: 0.2876040041446686\n900 Training loss: 0.30990901589393616\n910 Training loss: 0.15644805133342743\n920 Training loss: 0.23727695643901825\n930 Training loss: 0.15735198557376862\n940 Training loss: 0.18211178481578827\n950 Training loss: 0.3033396899700165\n960 Training loss: 0.2827306091785431\n970 Training loss: 0.27221009135246277\n980 Training loss: 0.23058487474918365\n990 Training loss: 0.2648584544658661\n1000 Training loss: 0.25830477476119995\n1010 Training loss: 0.30023571848869324\n1020 Training loss: 0.2643507122993469\n1030 Training loss: 0.1810612678527832\n1040 Training loss: 0.30582568049430847\n1050 Training loss: 0.272358238697052\n1060 Training loss: 0.41838181018829346\n1070 Training loss: 0.2688754200935364\n1080 Training loss: 0.23419040441513062\n1090 Training loss: 0.17643649876117706\n1100 Training loss: 0.1982138603925705\n1110 Training loss: 0.29706713557243347\n1120 Training loss: 0.3544333279132843\n1130 Training loss: 0.2553233206272125\n1140 Training loss: 0.3623139560222626\n1150 Training loss: 0.4167962968349457\n1160 Training loss: 0.24284397065639496\n1170 Training loss: 0.3131120800971985\n1180 Training loss: 0.2476487010717392\n1190 Training loss: 0.2893478274345398\n1200 Training loss: 0.2557072341442108\n1210 Training loss: 0.20343782007694244\n1220 Training loss: 0.23245035111904144\n1230 Training loss: 0.3818308711051941\n1240 Training loss: 0.22535543143749237\n1250 Training loss: 0.27543866634368896\n1260 Training loss: 0.3107246458530426\n1270 Training loss: 0.3138810098171234\n1280 Training loss: 0.2709321081638336\n1290 Training loss: 0.22128848731517792\n1300 Training loss: 0.32340481877326965\n1310 Training loss: 0.2386457324028015\n1320 Training loss: 0.25211647152900696\n1330 Training loss: 0.19345687329769135\n1340 Training loss: 0.23211830854415894\n1350 Training loss: 0.20626498758792877\n1360 Training loss: 0.19971340894699097\n1370 Training loss: 0.31462910771369934\n1380 Training loss: 0.3028547763824463\n1390 Training loss: 0.29009366035461426\n1400 Training loss: 0.43966686725616455\n1410 Training loss: 0.2097238302230835\n1420 Training loss: 0.249867245554924\n1430 Training loss: 0.23316513001918793\n1440 Training loss: 0.2512953579425812\n1450 Training loss: 0.3041386902332306\n1460 Training loss: 0.239576518535614\n1470 Training loss: 0.25786328315734863\n1480 Training loss: 0.2783592939376831\n1490 Training loss: 0.2988418936729431\n1500 Training loss: 0.22785373032093048\n1510 Training loss: 0.31392964720726013\n1520 Training loss: 0.35350531339645386\n1530 Training loss: 0.30349916219711304\n1540 Training loss: 0.3982069790363312\n1550 Training loss: 0.416462779045105\n1560 Training loss: 0.277946799993515\n1570 Training loss: 0.21119935810565948\n1580 Training loss: 0.28170835971832275\n1590 Training loss: 0.3258989453315735\n1600 Training loss: 0.28535357117652893\n1610 Training loss: 0.2657897472381592\n1620 Training loss: 0.2928721010684967\n1630 Training loss: 0.2535229027271271\n1640 Training loss: 0.2558891773223877\n1650 Training loss: 0.2170882672071457\n1660 Training loss: 0.22081132233142853\n1670 Training loss: 0.24416276812553406\n1680 Training loss: 0.33108001947402954\n1690 Training loss: 0.2628921568393707\n1700 Training loss: 0.19236396253108978\n1710 Training loss: 0.2723718583583832\n1720 Training loss: 0.1935722529888153\n1730 Training loss: 0.27574431896209717\n1740 Training loss: 0.2437736690044403\n1750 Training loss: 0.1733342707157135\n1760 Training loss: 0.2438746988773346\n1770 Training loss: 0.40010151267051697\n1780 Training loss: 0.2359766811132431\n1790 Training loss: 0.26460567116737366\n1800 Training loss: 0.24401801824569702\n1810 Training loss: 0.2628881335258484\n1820 Training loss: 0.3309086561203003\n1830 Training loss: 0.312852680683136\n1840 Training loss: 0.3397824466228485\n1850 Training loss: 0.27025356888771057\n1860 Training loss: 0.3766922652721405\n1870 Training loss: 0.3432108759880066\n1880 Training loss: 0.2360498458147049\n1890 Training loss: 0.18840740621089935\n1900 Training loss: 0.27059823274612427\n1910 Training loss: 0.24677728116512299\n1920 Training loss: 0.22569149732589722\n1930 Training loss: 0.21803291141986847\n1940 Training loss: 0.2645024359226227\n1950 Training loss: 0.2917534112930298\n1960 Training loss: 0.29270195960998535\n1970 Training loss: 0.2161373645067215\n1980 Training loss: 0.26974472403526306\n1990 Training loss: 0.2820934057235718\n2000 Training loss: 0.2500832676887512\n2010 Training loss: 0.24244995415210724\n2020 Training loss: 0.23993942141532898\n2030 Training loss: 0.24479632079601288\n2040 Training loss: 0.33825987577438354\n2050 Training loss: 0.2464931607246399\n2060 Training loss: 0.231657013297081\n2070 Training loss: 0.24237705767154694\n2080 Training loss: 0.278342604637146\n2090 Training loss: 0.24799104034900665\n2100 Training loss: 0.3117711544036865\n2110 Training loss: 0.3292507827281952\n2120 Training loss: 0.29796209931373596\n2130 Training loss: 0.22261101007461548\n2140 Training loss: 0.1631835401058197\n2150 Training loss: 0.20533235371112823\n2160 Training loss: 0.1658439040184021\n2170 Training loss: 0.20753803849220276\n2180 Training loss: 0.27595004439353943\n2190 Training loss: 0.37665870785713196\n2200 Training loss: 0.29278022050857544\n2210 Training loss: 0.154753178358078\n2220 Training loss: 0.2568782866001129\n2230 Training loss: 0.28220894932746887\n2240 Training loss: 0.2076021432876587\n2250 Training loss: 0.3032170236110687\n2260 Training loss: 0.30248120427131653\n2270 Training loss: 0.30061525106430054\n2280 Training loss: 0.2949044108390808\n2290 Training loss: 0.2526642084121704\n2300 Training loss: 0.22673369944095612\n2310 Training loss: 0.28895413875579834\n2320 Training loss: 0.3718826174736023\n2330 Training loss: 0.24441775679588318\n2340 Training loss: 0.22079528868198395\n2350 Training loss: 0.3155039846897125\n2360 Training loss: 0.2752062678337097\n2370 Training loss: 0.2737961709499359\n2380 Training loss: 0.3778630793094635\n2390 Training loss: 0.2568349242210388\n2400 Training loss: 0.2953382730484009\n2410 Training loss: 0.15869060158729553\n2420 Training loss: 0.22923824191093445\n2430 Training loss: 0.4065616726875305\n2440 Training loss: 0.20244953036308289\n2450 Training loss: 0.3040558695793152\n2460 Training loss: 0.25438231229782104\n2470 Training loss: 0.23364214599132538\n2480 Training loss: 0.2539370059967041\n2490 Training loss: 0.3092283010482788\n2500 Training loss: 0.27968600392341614\n2510 Training loss: 0.24522443115711212\n2520 Training loss: 0.22943517565727234\n2530 Training loss: 0.28693658113479614\n2540 Training loss: 0.2995339334011078\n2550 Training loss: 0.19825710356235504\n2560 Training loss: 0.17261941730976105\n2570 Training loss: 0.17911215126514435\n2580 Training loss: 0.2920492887496948\n2590 Training loss: 0.31541138887405396\n2600 Training loss: 0.25696322321891785\n2610 Training loss: 0.3112328350543976\n2620 Training loss: 0.2987174093723297\n2630 Training loss: 0.19538022577762604\n2640 Training loss: 0.22899778187274933\n2650 Training loss: 0.21520112454891205\n2660 Training loss: 0.22816605865955353\n2670 Training loss: 0.21271222829818726\n2680 Training loss: 0.33223870396614075\n2690 Training loss: 0.3220883905887604\n2700 Training loss: 0.22659170627593994\n2710 Training loss: 0.24100908637046814\n2720 Training loss: 0.2532374858856201\n2730 Training loss: 0.40259402990341187\n2740 Training loss: 0.24073581397533417\n2750 Training loss: 0.19854241609573364\n2760 Training loss: 0.3067227602005005\n2770 Training loss: 0.3858203589916229\n2780 Training loss: 0.24868632853031158\n2790 Training loss: 0.257246196269989\n2800 Training loss: 0.2732152044773102\n2810 Training loss: 0.2530469596385956\n2820 Training loss: 0.4040081799030304\n2830 Training loss: 0.26642170548439026\n2840 Training loss: 0.21632033586502075\n2850 Training loss: 0.23041890561580658\n2860 Training loss: 0.33505308628082275\n2870 Training loss: 0.2569577097892761\n2880 Training loss: 0.41532814502716064\n2890 Training loss: 0.26410356163978577\n2900 Training loss: 0.16898487508296967\n2910 Training loss: 0.23304298520088196\n2920 Training loss: 0.28329333662986755\n2930 Training loss: 0.4105921685695648\n2940 Training loss: 0.2052513062953949\n2950 Training loss: 0.4056697189807892\n2960 Training loss: 0.32763850688934326\n2970 Training loss: 0.2770823538303375\n2980 Training loss: 0.2480652928352356\n2990 Training loss: 0.2449483722448349\n3000 Training loss: 0.22208678722381592\n3010 Training loss: 0.30523404479026794\n3020 Training loss: 0.22679643332958221\n3030 Training loss: 0.2541622519493103\n3040 Training loss: 0.36313992738723755\n3050 Training loss: 0.28263235092163086\n3060 Training loss: 0.4251246750354767\n3070 Training loss: 0.2349257469177246\n3080 Training loss: 0.2513575255870819\n3090 Training loss: 0.2982496917247772\n3100 Training loss: 0.219053253531456\n3110 Training loss: 0.2487674504518509\n3120 Training loss: 0.2207661271095276\n3130 Training loss: 0.2417955994606018\n3140 Training loss: 0.2906312644481659\n3150 Training loss: 0.23104217648506165\n3160 Training loss: 0.31471318006515503\n3170 Training loss: 0.36134588718414307\n3180 Training loss: 0.2752378582954407\n3190 Training loss: 0.21393264830112457\n3200 Training loss: 0.478773832321167\n3210 Training loss: 0.29230207204818726\n3220 Training loss: 0.29364070296287537\n3230 Training loss: 0.21673060953617096\n3240 Training loss: 0.3210639953613281\n3250 Training loss: 0.30601149797439575\n3260 Training loss: 0.19893339276313782\n3270 Training loss: 0.28795313835144043\n3280 Training loss: 0.24271175265312195\n3290 Training loss: 0.22800883650779724\n3300 Training loss: 0.25964221358299255\n3310 Training loss: 0.31961897015571594\n3320 Training loss: 0.3044997453689575\n3330 Training loss: 0.2285865992307663\n3340 Training loss: 0.2864828109741211\n3350 Training loss: 0.26183393597602844\n3360 Training loss: 0.31969889998435974\n3370 Training loss: 0.3975166380405426\n3380 Training loss: 0.2684205174446106\n3390 Training loss: 0.2031547725200653\n3400 Training loss: 0.3033970892429352\n3410 Training loss: 0.2599512040615082\n3420 Training loss: 0.33728259801864624\n3430 Training loss: 0.2610524892807007\n3440 Training loss: 0.33112943172454834\n3450 Training loss: 0.33532705903053284\n3460 Training loss: 0.21224775910377502\n3470 Training loss: 0.2040850967168808\n3480 Training loss: 0.2693828046321869\n3490 Training loss: 0.2988172471523285\n3500 Training loss: 0.2990468740463257\n3510 Training loss: 0.26235440373420715\n3520 Training loss: 0.2654217481613159\n3530 Training loss: 0.2639562487602234\n3540 Training loss: 0.24713119864463806\n3550 Training loss: 0.23343442380428314\n3560 Training loss: 0.20540092885494232\n3570 Training loss: 0.2591277062892914\n3580 Training loss: 0.2809588313102722\n3590 Training loss: 0.2567785978317261\n3600 Training loss: 0.2240620255470276\n3610 Training loss: 0.3296279013156891\n3620 Training loss: 0.17661456763744354\n3630 Training loss: 0.23241344094276428\n3640 Training loss: 0.2963809370994568\n3650 Training loss: 0.21057434380054474\n3660 Training loss: 0.16743351519107819\n3670 Training loss: 0.20896059274673462\n3680 Training loss: 0.30045172572135925\n3690 Training loss: 0.24222300946712494\n3700 Training loss: 0.2710917294025421\n3710 Training loss: 0.2268507182598114\n3720 Training loss: 0.205786794424057\n3730 Training loss: 0.2000611275434494\n3740 Training loss: 0.27053943276405334\n3750 Training loss: 0.3031337261199951\n3760 Training loss: 0.3571028411388397\n3770 Training loss: 0.22529105842113495\n3780 Training loss: 0.1954219937324524\n3790 Training loss: 0.19087596237659454\n3800 Training loss: 0.35949787497520447\n3810 Training loss: 0.2759980857372284\n3820 Training loss: 0.2928101122379303\n3830 Training loss: 0.30097103118896484\n3840 Training loss: 0.21435187757015228\n3850 Training loss: 0.21459858119487762\n3860 Training loss: 0.20661066472530365\n3870 Training loss: 0.3118000030517578\n3880 Training loss: 0.23451882600784302\n3890 Training loss: 0.26804912090301514\n3900 Training loss: 0.2805444598197937\n3910 Training loss: 0.2674659788608551\n3920 Training loss: 0.2953961491584778\n3930 Training loss: 0.2800385355949402\n3940 Training loss: 0.2331910878419876\n3950 Training loss: 0.1881014108657837\n3960 Training loss: 0.2962300479412079\n3970 Training loss: 0.21244844794273376\n3980 Training loss: 0.22788313031196594\n3990 Training loss: 0.15532070398330688\n4000 Training loss: 0.29150956869125366\n4010 Training loss: 0.23136799037456512\n4020 Training loss: 0.41956180334091187\n4030 Training loss: 0.24807195365428925\n4040 Training loss: 0.29656141996383667\n4050 Training loss: 0.26182082295417786\n4060 Training loss: 0.24341796338558197\n4070 Training loss: 0.19830840826034546\n4080 Training loss: 0.25182878971099854\n4090 Training loss: 0.17618724703788757\n4100 Training loss: 0.3793909549713135\n4110 Training loss: 0.24634438753128052\n4120 Training loss: 0.21206854283809662\n4130 Training loss: 0.21608993411064148\n4140 Training loss: 0.2967037856578827\n4150 Training loss: 0.24487467110157013\n4160 Training loss: 0.2630746364593506\n4170 Training loss: 0.3581225872039795\n4180 Training loss: 0.24302010238170624\n4190 Training loss: 0.17405156791210175\n4200 Training loss: 0.3003527522087097\n4210 Training loss: 0.29824912548065186\n4220 Training loss: 0.2827555537223816\n4230 Training loss: 0.20619550347328186\n4240 Training loss: 0.23699216544628143\n4250 Training loss: 0.21372824907302856\n4260 Training loss: 0.26060187816619873\n4270 Training loss: 0.31708449125289917\n4280 Training loss: 0.2610173523426056\n4290 Training loss: 0.2195623219013214\n4300 Training loss: 0.28042203187942505\n4310 Training loss: 0.3582109212875366\n4320 Training loss: 0.26829251646995544\n4330 Training loss: 0.185576930642128\n4340 Training loss: 0.22675752639770508\n4350 Training loss: 0.2822107672691345\n4360 Training loss: 0.25205153226852417\n4370 Training loss: 0.26403892040252686\n4380 Training loss: 0.19932447373867035\n4390 Training loss: 0.18825750052928925\n4400 Training loss: 0.24443212151527405\n4410 Training loss: 0.21297632157802582\n4420 Training loss: 0.22898733615875244\n4430 Training loss: 0.22563348710536957\n4440 Training loss: 0.3062526285648346\n4450 Training loss: 0.27276307344436646\n4460 Training loss: 0.17559903860092163\n4470 Training loss: 0.23997989296913147\n4480 Training loss: 0.2593371272087097\n4490 Training loss: 0.2569963335990906\n4500 Training loss: 0.2831173837184906\n4510 Training loss: 0.5150266289710999\n4520 Training loss: 0.26699817180633545\n4530 Training loss: 0.24391578137874603\n4540 Training loss: 0.20286685228347778\n4550 Training loss: 0.18603050708770752\n4560 Training loss: 0.2642773985862732\n4570 Training loss: 0.19175300002098083\n4580 Training loss: 0.3487597405910492\n4590 Training loss: 0.2741965353488922\n4600 Training loss: 0.194443479180336\n4610 Training loss: 0.3211814761161804\n4620 Training loss: 0.21225495636463165\n4630 Training loss: 0.3535228669643402\n4640 Training loss: 0.26173412799835205\n4650 Training loss: 0.1693156659603119\n4660 Training loss: 0.2317589521408081\n4670 Training loss: 0.20538707077503204\n4680 Training loss: 0.2198987603187561\n4690 Training loss: 0.23234573006629944\n4700 Training loss: 0.21525153517723083\n4710 Training loss: 0.3441772162914276\n4720 Training loss: 0.20017802715301514\n4730 Training loss: 0.2783813774585724\n4740 Training loss: 0.2808088958263397\n4750 Training loss: 0.2995033860206604\n4760 Training loss: 0.20578907430171967\n4770 Training loss: 0.2405027598142624\n4780 Training loss: 0.3363106846809387\n4790 Training loss: 0.24662092328071594\n4800 Training loss: 0.19716882705688477\n4810 Training loss: 0.34901368618011475\n4820 Training loss: 0.16719843447208405\n4830 Training loss: 0.3124816119670868\n4840 Training loss: 0.20149801671504974\n4850 Training loss: 0.2195677012205124\n4860 Training loss: 0.2658212184906006\n4870 Training loss: 0.30522406101226807\n4880 Training loss: 0.2837626338005066\n4890 Training loss: 0.32641392946243286\n4900 Training loss: 0.29483911395072937\n4910 Training loss: 0.23648601770401\n4920 Training loss: 0.3024440407752991\n4930 Training loss: 0.293836772441864\n4940 Training loss: 0.32212623953819275\n4950 Training loss: 0.3883514702320099\n4960 Training loss: 0.27594059705734253\n4970 Training loss: 0.4307910203933716\n4980 Training loss: 0.2758289575576782\n4990 Training loss: 0.2912401556968689\n5000 Training loss: 0.3298007547855377\n5010 Training loss: 0.38390326499938965\n5020 Training loss: 0.31152573227882385\n5030 Training loss: 0.2348058670759201\n5040 Training loss: 0.2523823380470276\n5050 Training loss: 0.250697523355484\n5060 Training loss: 0.2672000229358673\n5070 Training loss: 0.2944867014884949\n5080 Training loss: 0.17574302852153778\n5090 Training loss: 0.23041920363903046\n5100 Training loss: 0.3280731439590454\n5110 Training loss: 0.2498418241739273\n5120 Training loss: 0.1842251718044281\n5130 Training loss: 0.24275651574134827\n5140 Training loss: 0.26321932673454285\n5150 Training loss: 0.2672693431377411\n5160 Training loss: 0.28202757239341736\n5170 Training loss: 0.2906252443790436\n5180 Training loss: 0.2663525640964508\n5190 Training loss: 0.2708600163459778\n5200 Training loss: 0.2678418457508087\n5210 Training loss: 0.2718106508255005\n5220 Training loss: 0.20722191035747528\n5230 Training loss: 0.23912151157855988\n5240 Training loss: 0.24669381976127625\n5250 Training loss: 0.25190988183021545\n5260 Training loss: 0.3874562084674835\n5270 Training loss: 0.2362837791442871\n5280 Training loss: 0.25773873925209045\n5290 Training loss: 0.18701252341270447\n5300 Training loss: 0.2529084086418152\n5310 Training loss: 0.23670099675655365\n5320 Training loss: 0.27729618549346924\n5330 Training loss: 0.3052324950695038\n5340 Training loss: 0.3096424639225006\n5350 Training loss: 0.2677925229072571\n5360 Training loss: 0.2679388225078583\n5370 Training loss: 0.17129378020763397\n5380 Training loss: 0.24913997948169708\n5390 Training loss: 0.3278777003288269\n5400 Training loss: 0.2554311156272888\n5410 Training loss: 0.208902969956398\n5420 Training loss: 0.26788967847824097\n5430 Training loss: 0.23964498937129974\n5440 Training loss: 0.37923189997673035\n5450 Training loss: 0.28297463059425354\n5460 Training loss: 0.28735625743865967\n5470 Training loss: 0.3411758542060852\n5480 Training loss: 0.22014905512332916\n5490 Training loss: 0.20025871694087982\n5500 Training loss: 0.24111320078372955\n5510 Training loss: 0.2426237314939499\n5520 Training loss: 0.27710554003715515\n5530 Training loss: 0.3302011787891388\n5540 Training loss: 0.2374865710735321\n5550 Training loss: 0.32597050070762634\n5560 Training loss: 0.2998226284980774\n5570 Training loss: 0.19122625887393951\n5580 Training loss: 0.34162643551826477\n5590 Training loss: 0.17497816681861877\n5600 Training loss: 0.2540800869464874\n5610 Training loss: 0.23219914734363556\n5620 Training loss: 0.2155047357082367\n5630 Training loss: 0.24452604353427887\n5640 Training loss: 0.18238584697246552\n5650 Training loss: 0.3252168595790863\n5660 Training loss: 0.24219158291816711\n5670 Training loss: 0.2722267210483551\n5680 Training loss: 0.2017863243818283\n5690 Training loss: 0.2537229657173157\n5700 Training loss: 0.24162809550762177\n5710 Training loss: 0.19893944263458252\n5720 Training loss: 0.24019350111484528\n5730 Training loss: 0.21110929548740387\n5740 Training loss: 0.22966979444026947\n5750 Training loss: 0.261050283908844\n5760 Training loss: 0.20141467452049255\n5770 Training loss: 0.20795674622058868\n5780 Training loss: 0.2494993805885315\n5790 Training loss: 0.25527846813201904\n5800 Training loss: 0.36817246675491333\n5810 Training loss: 0.3593258261680603\n5820 Training loss: 0.3043847978115082\n5830 Training loss: 0.2873155176639557\n5840 Training loss: 0.22263473272323608\n5850 Training loss: 0.2807053327560425\n5860 Training loss: 0.39316311478614807\n5870 Training loss: 0.27682197093963623\n5880 Training loss: 0.23879526555538177\n5890 Training loss: 0.22292432188987732\n5900 Training loss: 0.2039964348077774\n5910 Training loss: 0.38259026408195496\n5920 Training loss: 0.26531869173049927\n5930 Training loss: 0.16854411363601685\n5940 Training loss: 0.17497135698795319\n5950 Training loss: 0.2605797350406647\n5960 Training loss: 0.2322271764278412\n5970 Training loss: 0.2030935138463974\n5980 Training loss: 0.2513918876647949\n5990 Training loss: 0.3411559760570526\n6000 Training loss: 0.24710512161254883\n6010 Training loss: 0.26300060749053955\n6020 Training loss: 0.15374627709388733\n6030 Training loss: 0.2683558464050293\n6040 Training loss: 0.23150019347667694\n6050 Training loss: 0.22924217581748962\n6060 Training loss: 0.1956399381160736\n6070 Training loss: 0.23601806163787842\n6080 Training loss: 0.26948416233062744\n6090 Training loss: 0.18266303837299347\n6100 Training loss: 0.19471248984336853\n6110 Training loss: 0.2673718333244324\n6120 Training loss: 0.2603190541267395\n6130 Training loss: 0.3343101739883423\n6140 Training loss: 0.2796432375907898\n6150 Training loss: 0.18973910808563232\n6160 Training loss: 0.24498087167739868\n6170 Training loss: 0.23897023499011993\n6180 Training loss: 0.3416896164417267\n6190 Training loss: 0.279323548078537\n6200 Training loss: 0.18329139053821564\n6210 Training loss: 0.23849084973335266\n6220 Training loss: 0.21984153985977173\n6230 Training loss: 0.39481472969055176\n6240 Training loss: 0.28381776809692383\n6250 Training loss: 0.308358758687973\n6260 Training loss: 0.29962483048439026\n6270 Training loss: 0.21167542040348053\n6280 Training loss: 0.23302903771400452\n6290 Training loss: 0.19019053876399994\n6300 Training loss: 0.23379582166671753\n6310 Training loss: 0.2593806982040405\n6320 Training loss: 0.22699443995952606\n6330 Training loss: 0.20772697031497955\n6340 Training loss: 0.30946436524391174\n6350 Training loss: 0.25533661246299744\n6360 Training loss: 0.2326948493719101\n6370 Training loss: 0.1841079592704773\n6380 Training loss: 0.33890169858932495\n6390 Training loss: 0.3534846603870392\n6400 Training loss: 0.21397747099399567\n6410 Training loss: 0.18452316522598267\n6420 Training loss: 0.2584654986858368\n6430 Training loss: 0.2914031445980072\n6440 Training loss: 0.264530211687088\n6450 Training loss: 0.2830613851547241\n6460 Training loss: 0.23262223601341248\n6470 Training loss: 0.29985150694847107\n6480 Training loss: 0.31905597448349\n6490 Training loss: 0.20553523302078247\n6500 Training loss: 0.17430101335048676\n6510 Training loss: 0.2957993149757385\n6520 Training loss: 0.18243516981601715\n6530 Training loss: 0.33306485414505005\n6540 Training loss: 0.3480554521083832\n6550 Training loss: 0.2708836793899536\n6560 Training loss: 0.19545169174671173\n6570 Training loss: 0.2786969244480133\n6580 Training loss: 0.266129732131958\n6590 Training loss: 0.29765433073043823\n6600 Training loss: 0.2268589586019516\n6610 Training loss: 0.25783026218414307\n6620 Training loss: 0.21661968529224396\n6630 Training loss: 0.2558816075325012\n6640 Training loss: 0.20026440918445587\n6650 Training loss: 0.2864290773868561\n6660 Training loss: 0.19483734667301178\n6670 Training loss: 0.2796182930469513\n6680 Training loss: 0.24536046385765076\n6690 Training loss: 0.3106723725795746\n6700 Training loss: 0.36126986145973206\n6710 Training loss: 0.24788938462734222\n6720 Training loss: 0.25728175044059753\n6730 Training loss: 0.2396577000617981\n6740 Training loss: 0.27216213941574097\n6750 Training loss: 0.17660844326019287\n6760 Training loss: 0.17410624027252197\n6770 Training loss: 0.21818746626377106\n6780 Training loss: 0.24829556047916412\n6790 Training loss: 0.3300280272960663\n6800 Training loss: 0.2778753638267517\n6810 Training loss: 0.2109798640012741\n6820 Training loss: 0.2812795639038086\n6830 Training loss: 0.2684386372566223\n6840 Training loss: 0.1628313511610031\n6850 Training loss: 0.2674829661846161\n6860 Training loss: 0.4643931984901428\n6870 Training loss: 0.21495653688907623\n6880 Training loss: 0.26112881302833557\n6890 Training loss: 0.1942024827003479\n6900 Training loss: 0.2489476054906845\n6910 Training loss: 0.3012715280056\n6920 Training loss: 0.2630636990070343\n6930 Training loss: 0.2899395823478699\n6940 Training loss: 0.2877838909626007\n6950 Training loss: 0.2588823437690735\n6960 Training loss: 0.238594189286232\n6970 Training loss: 0.2084944099187851\n6980 Training loss: 0.27798163890838623\n6990 Training loss: 0.20865659415721893\n7000 Training loss: 0.20487207174301147\n7010 Training loss: 0.2219366878271103\n7020 Training loss: 0.2269245684146881\n7030 Training loss: 0.33002811670303345\n7040 Training loss: 0.20221814513206482\n7050 Training loss: 0.1809033751487732\n7060 Training loss: 0.3273560404777527\n7070 Training loss: 0.25359150767326355\n7080 Training loss: 0.21759776771068573\n7090 Training loss: 0.19269336760044098\n7100 Training loss: 0.1614164113998413\n7110 Training loss: 0.2993774116039276\n7120 Training loss: 0.19309863448143005\n7130 Training loss: 0.3574903905391693\n7140 Training loss: 0.29812195897102356\n7150 Training loss: 0.26168206334114075\n7160 Training loss: 0.2220531404018402\n7170 Training loss: 0.253341943025589\n7180 Training loss: 0.3016311526298523\n7190 Training loss: 0.2601653039455414\n7200 Training loss: 0.2529110610485077\n7210 Training loss: 0.337157279253006\n7220 Training loss: 0.25919443368911743\n7230 Training loss: 0.18782855570316315\n7240 Training loss: 0.2989130914211273\n7250 Training loss: 0.295032262802124\n7260 Training loss: 0.2980363368988037\n7270 Training loss: 0.2126169055700302\n7280 Training loss: 0.16821376979351044\n7290 Training loss: 0.2933517396450043\n7300 Training loss: 0.22600482404232025\n7310 Training loss: 0.2921534776687622\n7320 Training loss: 0.15977156162261963\n7330 Training loss: 0.2511940598487854\n7340 Training loss: 0.24084140360355377\n7350 Training loss: 0.2553091049194336\n7360 Training loss: 0.21716907620429993\n7370 Training loss: 0.21782568097114563\n7380 Training loss: 0.3368511199951172\n7390 Training loss: 0.22051264345645905\n7400 Training loss: 0.26241615414619446\n7410 Training loss: 0.29805970191955566\n7420 Training loss: 0.20948272943496704\n7430 Training loss: 0.24413053691387177\n7440 Training loss: 0.15290777385234833\n7450 Training loss: 0.16974562406539917\n7460 Training loss: 0.23991914093494415\n7470 Training loss: 0.29051345586776733\n7480 Training loss: 0.22494138777256012\n7490 Training loss: 0.187527135014534\n7500 Training loss: 0.2125745713710785\n7510 Training loss: 0.2612629532814026\n7520 Training loss: 0.2572021186351776\n7530 Training loss: 0.2797520160675049\n7540 Training loss: 0.18702511489391327\n7550 Training loss: 0.24709109961986542\n7560 Training loss: 0.2829405665397644\n7570 Training loss: 0.3160702884197235\n7580 Training loss: 0.28053295612335205\n7590 Training loss: 0.2948343753814697\n7600 Training loss: 0.2570067346096039\n7610 Training loss: 0.23339176177978516\n7620 Training loss: 0.30980029702186584\n7630 Training loss: 0.2732802927494049\n7640 Training loss: 0.26881125569343567\n7650 Training loss: 0.24497653543949127\n7660 Training loss: 0.36466026306152344\n7670 Training loss: 0.25672072172164917\n7680 Training loss: 0.20634044706821442\n7690 Training loss: 0.23495103418827057\n7700 Training loss: 0.2576453387737274\n7710 Training loss: 0.17364045977592468\n7720 Training loss: 0.24928884208202362\n7730 Training loss: 0.24598339200019836\n7740 Training loss: 0.2234632521867752\n7750 Training loss: 0.19834066927433014\n7760 Training loss: 0.22281311452388763\n7770 Training loss: 0.1688128262758255\n7780 Training loss: 0.3214956223964691\n7790 Training loss: 0.436445951461792\n7800 Training loss: 0.2108076512813568\n7810 Training loss: 0.2570190727710724\n7820 Training loss: 0.25291192531585693\n7830 Training loss: 0.18935394287109375\n7840 Training loss: 0.24966855347156525\n7850 Training loss: 0.27364328503608704\n7860 Training loss: 0.21151894330978394\n7870 Training loss: 0.2684388756752014\n7880 Training loss: 0.22064098715782166\n7890 Training loss: 0.19964876770973206\n7900 Training loss: 0.2282383143901825\n7910 Training loss: 0.2592073678970337\n7920 Training loss: 0.21495477855205536\n7930 Training loss: 0.17488200962543488\n7940 Training loss: 0.20101457834243774\n7950 Training loss: 0.2608860731124878\n7960 Training loss: 0.24381296336650848\n7970 Training loss: 0.2543058693408966\n7980 Training loss: 0.27775466442108154\n7990 Training loss: 0.2956955134868622\n8000 Training loss: 0.20496012270450592\n8010 Training loss: 0.2634979486465454\n8020 Training loss: 0.1844082772731781\n8030 Training loss: 0.20877736806869507\n8040 Training loss: 0.29844561219215393\n8050 Training loss: 0.2818218171596527\n8060 Training loss: 0.30090221762657166\n8070 Training loss: 0.27029797434806824\n8080 Training loss: 0.2686929404735565\n8090 Training loss: 0.31015750765800476\n8100 Training loss: 0.17768114805221558\n8110 Training loss: 0.3907293379306793\n8120 Training loss: 0.23886243999004364\n8130 Training loss: 0.20051971077919006\n8140 Training loss: 0.21686120331287384\n8150 Training loss: 0.24404916167259216\n8160 Training loss: 0.28171291947364807\n8170 Training loss: 0.23116110265254974\n8180 Training loss: 0.23826389014720917\n8190 Training loss: 0.19825157523155212\n8200 Training loss: 0.2560324966907501\n8210 Training loss: 0.35207465291023254\n8220 Training loss: 0.1916382759809494\n8230 Training loss: 0.21398380398750305\n8240 Training loss: 0.22480115294456482\n8250 Training loss: 0.2731934189796448\n8260 Training loss: 0.2520843744277954\n8270 Training loss: 0.1822029948234558\n8280 Training loss: 0.242757648229599\n8290 Training loss: 0.22590294480323792\n8300 Training loss: 0.2600454092025757\n8310 Training loss: 0.1912676990032196\n8320 Training loss: 0.3140454590320587\n8330 Training loss: 0.1911558210849762\n8340 Training loss: 0.19543331861495972\n8350 Training loss: 0.1953120231628418\n8360 Training loss: 0.18998411297798157\n8370 Training loss: 0.23836025595664978\n8380 Training loss: 0.2269870787858963\n8390 Training loss: 0.15631236135959625\n8400 Training loss: 0.20310312509536743\n8410 Training loss: 0.1999669224023819\n8420 Training loss: 0.2362089902162552\n8430 Training loss: 0.2550917863845825\n8440 Training loss: 0.2253141552209854\n8450 Training loss: 0.2378385365009308\n8460 Training loss: 0.20056436955928802\n8470 Training loss: 0.2858043909072876\n8480 Training loss: 0.19537341594696045\n8490 Training loss: 0.24298083782196045\n8500 Training loss: 0.2641282379627228\n8510 Training loss: 0.25502684712409973\n8520 Training loss: 0.2303175926208496\n8530 Training loss: 0.2951924800872803\n8540 Training loss: 0.2320534586906433\n8550 Training loss: 0.29944735765457153\n8560 Training loss: 0.3151681423187256\n8570 Training loss: 0.3100683093070984\n8580 Training loss: 0.22467389702796936\n8590 Training loss: 0.30425941944122314\n8600 Training loss: 0.3142073154449463\n8610 Training loss: 0.27774250507354736\n8620 Training loss: 0.18166616559028625\n8630 Training loss: 0.23520036041736603\n8640 Training loss: 0.2941839396953583\n8650 Training loss: 0.22891414165496826\n8660 Training loss: 0.22852760553359985\n8670 Training loss: 0.3124961256980896\n8680 Training loss: 0.22554431855678558\n8690 Training loss: 0.33756759762763977\n8700 Training loss: 0.16283240914344788\n8710 Training loss: 0.24172498285770416\n8720 Training loss: 0.2051783800125122\n8730 Training loss: 0.2897239923477173\n8740 Training loss: 0.24760374426841736\n8750 Training loss: 0.253094345331192\n8760 Training loss: 0.2487565577030182\n8770 Training loss: 0.1616484671831131\n8780 Training loss: 0.23706136643886566\n8790 Training loss: 0.27579817175865173\n8800 Training loss: 0.3104143738746643\n8810 Training loss: 0.2564002275466919\n8820 Training loss: 0.2599777281284332\n8830 Training loss: 0.23943409323692322\n8840 Training loss: 0.18635143339633942\n8850 Training loss: 0.1837066262960434\n8860 Training loss: 0.1951437145471573\n8870 Training loss: 0.2530473470687866\n8880 Training loss: 0.2781616449356079\n8890 Training loss: 0.3194035589694977\n8900 Training loss: 0.2927073538303375\n8910 Training loss: 0.2027105689048767\n8920 Training loss: 0.21837802231311798\n8930 Training loss: 0.238124281167984\n8940 Training loss: 0.20899751782417297\n8950 Training loss: 0.18314534425735474\n8960 Training loss: 0.37854740023612976\n8970 Training loss: 0.18703240156173706\n8980 Training loss: 0.2441902905702591\n8990 Training loss: 0.23805779218673706\n9000 Training loss: 0.1489514410495758\n9010 Training loss: 0.23962944746017456\n9020 Training loss: 0.3352220058441162\n9030 Training loss: 0.23493243753910065\n9040 Training loss: 0.3023260235786438\n9050 Training loss: 0.18619811534881592\n9060 Training loss: 0.2521969676017761\n9070 Training loss: 0.19135023653507233\n9080 Training loss: 0.31032267212867737\n9090 Training loss: 0.2565065622329712\n9100 Training loss: 0.2472229152917862\n9110 Training loss: 0.20373724400997162\n9120 Training loss: 0.3106818199157715\n9130 Training loss: 0.3107350170612335\n9140 Training loss: 0.2853127419948578\n9150 Training loss: 0.17336054146289825\n9160 Training loss: 0.12102218717336655\n9170 Training loss: 0.20318248867988586\n9180 Training loss: 0.3054095506668091\n9190 Training loss: 0.20816852152347565\n9200 Training loss: 0.2944416105747223\n9210 Training loss: 0.2860153615474701\n9220 Training loss: 0.25974613428115845\n9230 Training loss: 0.2653311491012573\n9240 Training loss: 0.15655581653118134\n9250 Training loss: 0.3187648057937622\n9260 Training loss: 0.27344080805778503\n9270 Training loss: 0.16982196271419525\n9280 Training loss: 0.20909814536571503\n9290 Training loss: 0.12003979086875916\n9300 Training loss: 0.24527759850025177\n9310 Training loss: 0.20885227620601654\n9320 Training loss: 0.24334600567817688\n9330 Training loss: 0.25405558943748474\n9340 Training loss: 0.18505945801734924\n9350 Training loss: 0.3384087085723877\n9360 Training loss: 0.32035958766937256\n9370 Training loss: 0.22560438513755798\n9380 Training loss: 0.21166455745697021\n9390 Training loss: 0.2033742517232895\n9400 Training loss: 0.2619042694568634\n9410 Training loss: 0.2717157304286957\n9420 Training loss: 0.21188199520111084\n9430 Training loss: 0.28049948811531067\n9440 Training loss: 0.2111775428056717\n9450 Training loss: 0.2752628028392792\n9460 Training loss: 0.24712993204593658\n9470 Training loss: 0.2529417872428894\n9480 Training loss: 0.21869514882564545\n9490 Training loss: 0.19141532480716705\n9500 Training loss: 0.2274760752916336\n9510 Training loss: 0.21279166638851166\n9520 Training loss: 0.25428903102874756\n9530 Training loss: 0.22610624134540558\n9540 Training loss: 0.24988475441932678\n9550 Training loss: 0.2162741720676422\n9560 Training loss: 0.2873442471027374\n9570 Training loss: 0.1934598982334137\n9580 Training loss: 0.1763961762189865\n9590 Training loss: 0.23727744817733765\n9600 Training loss: 0.21643753349781036\n9610 Training loss: 0.1689639538526535\n9620 Training loss: 0.2602670192718506\n9630 Training loss: 0.22285230457782745\n9640 Training loss: 0.2383006513118744\n9650 Training loss: 0.25414353609085083\n9660 Training loss: 0.26501011848449707\n9670 Training loss: 0.3553990125656128\n9680 Training loss: 0.2584044635295868\n9690 Training loss: 0.2600228190422058\n9700 Training loss: 0.21704693138599396\n9710 Training loss: 0.3112308979034424\n9720 Training loss: 0.21352353692054749\n9730 Training loss: 0.2846630811691284\n9740 Training loss: 0.2990277409553528\n9750 Training loss: 0.16653259098529816\n9760 Training loss: 0.21114999055862427\n9770 Training loss: 0.2886632978916168\n9780 Training loss: 0.2042689472436905\n9790 Training loss: 0.21154627203941345\n9800 Training loss: 0.2967120110988617\n9810 Training loss: 0.2768157124519348\n9820 Training loss: 0.1996341347694397\n9830 Training loss: 0.22692103683948517\n9840 Training loss: 0.31298333406448364\n9850 Training loss: 0.26001954078674316\n9860 Training loss: 0.30355626344680786\n9870 Training loss: 0.22865355014801025\n9880 Training loss: 0.28338494896888733\n9890 Training loss: 0.2423093020915985\n9900 Training loss: 0.1394721418619156\n9910 Training loss: 0.28448837995529175\n9920 Training loss: 0.22269222140312195\n9930 Training loss: 0.20629414916038513\n9940 Training loss: 0.20284657180309296\n9950 Training loss: 0.29532620310783386\n9960 Training loss: 0.1994587481021881\n9970 Training loss: 0.27428868412971497\n9980 Training loss: 0.132330983877182\n9990 Training loss: 0.3468407094478607\n10000 Training loss: 0.2583617866039276\n10010 Training loss: 0.17635707557201385\n10020 Training loss: 0.22413916885852814\n10030 Training loss: 0.3135886788368225\n10040 Training loss: 0.22085809707641602\n10050 Training loss: 0.21450656652450562\n10060 Training loss: 0.19857467710971832\n10070 Training loss: 0.183182030916214\n10080 Training loss: 0.21152399480342865\n10090 Training loss: 0.1774168759584427\n10100 Training loss: 0.2279466688632965\n10110 Training loss: 0.33721423149108887\n10120 Training loss: 0.23702430725097656\n10130 Training loss: 0.23595866560935974\n10140 Training loss: 0.1890576332807541\n10150 Training loss: 0.2542315423488617\n10160 Training loss: 0.23118610680103302\n10170 Training loss: 0.20816528797149658\n10180 Training loss: 0.17121486365795135\n10190 Training loss: 0.2615458071231842\n10200 Training loss: 0.2371709793806076\n10210 Training loss: 0.3285183608531952\n10220 Training loss: 0.24691936373710632\n10230 Training loss: 0.31054216623306274\n10240 Training loss: 0.21142680943012238\n10250 Training loss: 0.23468536138534546\n10260 Training loss: 0.28840383887290955\n10270 Training loss: 0.38105517625808716\n10280 Training loss: 0.21191424131393433\n10290 Training loss: 0.20268714427947998\n10300 Training loss: 0.2778131365776062\n10310 Training loss: 0.24758405983448029\n10320 Training loss: 0.1954793483018875\n10330 Training loss: 0.25434359908103943\n10340 Training loss: 0.2507314682006836\n10350 Training loss: 0.27485978603363037\n10360 Training loss: 0.2607105076313019\n10370 Training loss: 0.2810060679912567\n10380 Training loss: 0.2510257959365845\n10390 Training loss: 0.28587132692337036\n10400 Training loss: 0.32932746410369873\n10410 Training loss: 0.2942359447479248\n10420 Training loss: 0.18280337750911713\n10430 Training loss: 0.1678602248430252\n10440 Training loss: 0.3239114284515381\n10450 Training loss: 0.20947735011577606\n10460 Training loss: 0.23479461669921875\n10470 Training loss: 0.27101266384124756\n10480 Training loss: 0.236983522772789\n10490 Training loss: 0.2268800586462021\n10500 Training loss: 0.25313013792037964\n10510 Training loss: 0.19706591963768005\n10520 Training loss: 0.18876150250434875\n10530 Training loss: 0.25686585903167725\n10540 Training loss: 0.29597270488739014\n10550 Training loss: 0.2932652533054352\n10560 Training loss: 0.3310338258743286\n10570 Training loss: 0.2558079957962036\n10580 Training loss: 0.2575913369655609\n10590 Training loss: 0.16853636503219604\n10600 Training loss: 0.32624390721321106\n10610 Training loss: 0.2607116103172302\n10620 Training loss: 0.3817988932132721\n10630 Training loss: 0.2850126028060913\n10640 Training loss: 0.2800822854042053\n10650 Training loss: 0.2762547433376312\n10660 Training loss: 0.18870706856250763\n10670 Training loss: 0.21250692009925842\n10680 Training loss: 0.16221536695957184\n10690 Training loss: 0.2201119214296341\n10700 Training loss: 0.2042725384235382\n10710 Training loss: 0.33160194754600525\n10720 Training loss: 0.193486750125885\n10730 Training loss: 0.19710507988929749\n10740 Training loss: 0.185161292552948\n10750 Training loss: 0.21754242479801178\n10760 Training loss: 0.18222713470458984\n10770 Training loss: 0.23844964802265167\n10780 Training loss: 0.19875197112560272\n10790 Training loss: 0.20118951797485352\n10800 Training loss: 0.2732042074203491\n10810 Training loss: 0.24369405210018158\n10820 Training loss: 0.25354790687561035\n10830 Training loss: 0.24036870896816254\n10840 Training loss: 0.32271191477775574\n10850 Training loss: 0.20652389526367188\n10860 Training loss: 0.202782541513443\n10870 Training loss: 0.3172365725040436\n10880 Training loss: 0.26011019945144653\n10890 Training loss: 0.21200093626976013\n10900 Training loss: 0.24993237853050232\n10910 Training loss: 0.24662603437900543\n10920 Training loss: 0.16943156719207764\n10930 Training loss: 0.1920480728149414\n10940 Training loss: 0.23509575426578522\n10950 Training loss: 0.2241905778646469\n10960 Training loss: 0.24357345700263977\n10970 Training loss: 0.19269028306007385\n10980 Training loss: 0.1322762370109558\n10990 Training loss: 0.17651161551475525\n11000 Training loss: 0.2422330230474472\n11010 Training loss: 0.24112491309642792\n11020 Training loss: 0.22902752459049225\n11030 Training loss: 0.26574957370758057\n11040 Training loss: 0.21637067198753357\n11050 Training loss: 0.30738791823387146\n11060 Training loss: 0.2853214740753174\n11070 Training loss: 0.12975914776325226\n11080 Training loss: 0.24489693343639374\n11090 Training loss: 0.3162906765937805\n11100 Training loss: 0.23132222890853882\n11110 Training loss: 0.2795622646808624\n11120 Training loss: 0.20012351870536804\n11130 Training loss: 0.25669822096824646\n11140 Training loss: 0.20343774557113647\n11150 Training loss: 0.2668760120868683\n11160 Training loss: 0.3254304826259613\n11170 Training loss: 0.20357002317905426\n11180 Training loss: 0.1957571655511856\n11190 Training loss: 0.15538142621517181\n11200 Training loss: 0.2683285176753998\n11210 Training loss: 0.22830158472061157\n11220 Training loss: 0.2472543716430664\n11230 Training loss: 0.20331226289272308\n11240 Training loss: 0.3489971458911896\n11250 Training loss: 0.26841431856155396\n11260 Training loss: 0.19027680158615112\n11270 Training loss: 0.25427335500717163\n11280 Training loss: 0.2329072207212448\n11290 Training loss: 0.19323641061782837\n11300 Training loss: 0.20783846080303192\n11310 Training loss: 0.20089079439640045\n11320 Training loss: 0.3239835500717163\n11330 Training loss: 0.2956817150115967\n11340 Training loss: 0.2345246821641922\n11350 Training loss: 0.2699044346809387\n11360 Training loss: 0.2121322602033615\n11370 Training loss: 0.21156719326972961\n11380 Training loss: 0.1899339258670807\n11390 Training loss: 0.26889729499816895\n11400 Training loss: 0.19419433176517487\n11410 Training loss: 0.16628669202327728\n11420 Training loss: 0.20183821022510529\n11430 Training loss: 0.30679500102996826\n11440 Training loss: 0.19574391841888428\n11450 Training loss: 0.33163031935691833\n11460 Training loss: 0.23769322037696838\n11470 Training loss: 0.30397629737854004\n11480 Training loss: 0.20054073631763458\n11490 Training loss: 0.1873316764831543\n11500 Training loss: 0.17725545167922974\n11510 Training loss: 0.23461684584617615\n11520 Training loss: 0.2584528625011444\n11530 Training loss: 0.20695370435714722\n11540 Training loss: 0.24166704714298248\n11550 Training loss: 0.27387312054634094\n11560 Training loss: 0.225217804312706\n11570 Training loss: 0.2861086130142212\n11580 Training loss: 0.2380167543888092\n11590 Training loss: 0.2204309105873108\n11600 Training loss: 0.21010355651378632\n11610 Training loss: 0.24786269664764404\n11620 Training loss: 0.2486162930727005\n11630 Training loss: 0.2909654676914215\n11640 Training loss: 0.26727479696273804\n11650 Training loss: 0.23219798505306244\n11660 Training loss: 0.23558002710342407\n11670 Training loss: 0.17759771645069122\n11680 Training loss: 0.2187945544719696\n11690 Training loss: 0.3115099370479584\n11700 Training loss: 0.2690414488315582\n11710 Training loss: 0.2636104226112366\n11720 Training loss: 0.20722892880439758\n11730 Training loss: 0.23255129158496857\n11740 Training loss: 0.26817232370376587\n11750 Training loss: 0.24011334776878357\n11760 Training loss: 0.21188941597938538\n11770 Training loss: 0.2605462372303009\n11780 Training loss: 0.22108949720859528\n11790 Training loss: 0.24551604688167572\n11800 Training loss: 0.27952951192855835\n11810 Training loss: 0.19771187007427216\n11820 Training loss: 0.3294977843761444\n11830 Training loss: 0.24372616410255432\n11840 Training loss: 0.36756157875061035\n11850 Training loss: 0.26145139336586\n11860 Training loss: 0.2229176163673401\n11870 Training loss: 0.29428717494010925\n11880 Training loss: 0.23568212985992432\n11890 Training loss: 0.12441621720790863\n11900 Training loss: 0.22009973227977753\n11910 Training loss: 0.18580245971679688\n11920 Training loss: 0.2853805124759674\n11930 Training loss: 0.46676716208457947\n11940 Training loss: 0.26760542392730713\n11950 Training loss: 0.236151784658432\n11960 Training loss: 0.22095856070518494\n11970 Training loss: 0.17310284078121185\n11980 Training loss: 0.3506821095943451\n11990 Training loss: 0.30016541481018066\n12000 Training loss: 0.24543975293636322\n12010 Training loss: 0.31060755252838135\n12020 Training loss: 0.2148115336894989\n12030 Training loss: 0.23093470931053162\n12040 Training loss: 0.2303217053413391\n12050 Training loss: 0.2583024799823761\n12060 Training loss: 0.22933083772659302\n12070 Training loss: 0.3077572286128998\n12080 Training loss: 0.25171032547950745\n12090 Training loss: 0.267693430185318\n12100 Training loss: 0.18636764585971832\n12110 Training loss: 0.1851639747619629\n12120 Training loss: 0.22905847430229187\n12130 Training loss: 0.24106507003307343\n12140 Training loss: 0.19163469970226288\n12150 Training loss: 0.1812242865562439\n12160 Training loss: 0.23832973837852478\n12170 Training loss: 0.19530221819877625\n12180 Training loss: 0.2781834304332733\n12190 Training loss: 0.20625442266464233\n12200 Training loss: 0.1917683482170105\n12210 Training loss: 0.24385814368724823\n12220 Training loss: 0.20041699707508087\n12230 Training loss: 0.21112321317195892\n12240 Training loss: 0.2669500410556793\n12250 Training loss: 0.23830558359622955\n12260 Training loss: 0.2994531989097595\n12270 Training loss: 0.18597465753555298\n12280 Training loss: 0.2781955897808075\n12290 Training loss: 0.3252590596675873\n12300 Training loss: 0.32468071579933167\n12310 Training loss: 0.2347329556941986\n12320 Training loss: 0.21484018862247467\n12330 Training loss: 0.270369291305542\n12340 Training loss: 0.27058127522468567\n12350 Training loss: 0.24266520142555237\n12360 Training loss: 0.23452499508857727\n12370 Training loss: 0.17051860690116882\n12380 Training loss: 0.27684497833251953\n12390 Training loss: 0.22421449422836304\n12400 Training loss: 0.2932437062263489\n12410 Training loss: 0.16593319177627563\n12420 Training loss: 0.20790992677211761\n12430 Training loss: 0.21937227249145508\n12440 Training loss: 0.3023717403411865\n12450 Training loss: 0.33166706562042236\n12460 Training loss: 0.24715057015419006\n12470 Training loss: 0.1806858479976654\n12480 Training loss: 0.28827038407325745\n12490 Training loss: 0.19106462597846985\n12500 Training loss: 0.21553024649620056\n12510 Training loss: 0.24114084243774414\n12520 Training loss: 0.2726101875305176\n12530 Training loss: 0.34956616163253784\n12540 Training loss: 0.24989010393619537\n12550 Training loss: 0.20883655548095703\n12560 Training loss: 0.1785983145236969\n12570 Training loss: 0.2873379588127136\n12580 Training loss: 0.2515909671783447\n12590 Training loss: 0.23228172957897186\n12600 Training loss: 0.23239749670028687\n12610 Training loss: 0.252465158700943\n12620 Training loss: 0.3486795425415039\n12630 Training loss: 0.1470702737569809\n12640 Training loss: 0.23361824452877045\n12650 Training loss: 0.24933978915214539\n12660 Training loss: 0.31533122062683105\n12670 Training loss: 0.1535649448633194\n12680 Training loss: 0.22623151540756226\n12690 Training loss: 0.28437912464141846\n12700 Training loss: 0.2766244411468506\n12710 Training loss: 0.23672567307949066\n12720 Training loss: 0.2627362012863159\n12730 Training loss: 0.21205995976924896\n12740 Training loss: 0.17271198332309723\n12750 Training loss: 0.19587194919586182\n12760 Training loss: 0.25421059131622314\n12770 Training loss: 0.21580691635608673\n12780 Training loss: 0.31780603528022766\n12790 Training loss: 0.2578869163990021\n12800 Training loss: 0.20737451314926147\n12810 Training loss: 0.22945527732372284\n12820 Training loss: 0.29209113121032715\n12830 Training loss: 0.1960989236831665\n12840 Training loss: 0.22561997175216675\n12850 Training loss: 0.2652193605899811\n12860 Training loss: 0.25016337633132935\n12870 Training loss: 0.23633113503456116\n12880 Training loss: 0.3231014907360077\n12890 Training loss: 0.2585533857345581\n12900 Training loss: 0.21358579397201538\n12910 Training loss: 0.34141263365745544\n12920 Training loss: 0.2240297645330429\n12930 Training loss: 0.17978860437870026\n12940 Training loss: 0.29935988783836365\n12950 Training loss: 0.14506036043167114\n12960 Training loss: 0.16200374066829681\n12970 Training loss: 0.160392627120018\n12980 Training loss: 0.18371307849884033\n12990 Training loss: 0.11930658668279648\n13000 Training loss: 0.17470556497573853\n13010 Training loss: 0.15235796570777893\n13020 Training loss: 0.3189932703971863\n13030 Training loss: 0.20260277390480042\n13040 Training loss: 0.2119993269443512\n13050 Training loss: 0.23944391310214996\n13060 Training loss: 0.2863966226577759\n13070 Training loss: 0.29282906651496887\n13080 Training loss: 0.3300515115261078\n13090 Training loss: 0.2849550247192383\n13100 Training loss: 0.28073492646217346\n13110 Training loss: 0.25547024607658386\n13120 Training loss: 0.21745212376117706\n13130 Training loss: 0.182374969124794\n13140 Training loss: 0.295126348733902\n13150 Training loss: 0.3342680037021637\n13160 Training loss: 0.22283655405044556\n13170 Training loss: 0.2677881717681885\n13180 Training loss: 0.18367312848567963\n13190 Training loss: 0.1994401514530182\n13200 Training loss: 0.27094799280166626\n13210 Training loss: 0.21109862625598907\n13220 Training loss: 0.2835264205932617\n13230 Training loss: 0.18367397785186768\n13240 Training loss: 0.20390740036964417\n13250 Training loss: 0.23755745589733124\n13260 Training loss: 0.26396074891090393\n13270 Training loss: 0.303811639547348\n13280 Training loss: 0.19515454769134521\n13290 Training loss: 0.27615973353385925\n13300 Training loss: 0.15498638153076172\n13310 Training loss: 0.23774556815624237\n13320 Training loss: 0.176416277885437\n13330 Training loss: 0.21817989647388458\n13340 Training loss: 0.2436213493347168\n13350 Training loss: 0.2102559208869934\n13360 Training loss: 0.2787233293056488\n13370 Training loss: 0.18498975038528442\n13380 Training loss: 0.2606000602245331\n13390 Training loss: 0.28380316495895386\n13400 Training loss: 0.27873459458351135\n13410 Training loss: 0.24526500701904297\n13420 Training loss: 0.2377038449048996\n13430 Training loss: 0.22180280089378357\n13440 Training loss: 0.28933992981910706\n13450 Training loss: 0.1674584597349167\n13460 Training loss: 0.25885745882987976\n13470 Training loss: 0.18434496223926544\n13480 Training loss: 0.1910061240196228\n13490 Training loss: 0.2532919645309448\n13500 Training loss: 0.21351535618305206\n13510 Training loss: 0.32955101132392883\n13520 Training loss: 0.18611297011375427\n13530 Training loss: 0.2978103756904602\n13540 Training loss: 0.21510469913482666\n13550 Training loss: 0.3310156762599945\n13560 Training loss: 0.18148979544639587\n13570 Training loss: 0.3011884391307831\n13580 Training loss: 0.21799910068511963\n13590 Training loss: 0.1962379813194275\n13600 Training loss: 0.18693776428699493\n13610 Training loss: 0.2548564672470093\n13620 Training loss: 0.2151709347963333\n13630 Training loss: 0.2676820456981659\n13640 Training loss: 0.22640417516231537\n13650 Training loss: 0.21976566314697266\n13660 Training loss: 0.18017226457595825\n13670 Training loss: 0.21181631088256836\n13680 Training loss: 0.19593647122383118\n13690 Training loss: 0.21365267038345337\n13700 Training loss: 0.243983194231987\n13710 Training loss: 0.19779498875141144\n13720 Training loss: 0.2639879882335663\n13730 Training loss: 0.2859684228897095\n13740 Training loss: 0.3252103328704834\n13750 Training loss: 0.19996514916419983\n13760 Training loss: 0.28910306096076965\n13770 Training loss: 0.23564624786376953\n13780 Training loss: 0.1946878880262375\n13790 Training loss: 0.22591790556907654\n13800 Training loss: 0.1796627789735794\n13810 Training loss: 0.19215020537376404\n13820 Training loss: 0.19976527988910675\n13830 Training loss: 0.18212401866912842\n13840 Training loss: 0.26624730229377747\n13850 Training loss: 0.2630791962146759\n13860 Training loss: 0.21158292889595032\n13870 Training loss: 0.21943697333335876\n13880 Training loss: 0.20987389981746674\n13890 Training loss: 0.24172092974185944\n13900 Training loss: 0.23615364730358124\n13910 Training loss: 0.22799034416675568\n13920 Training loss: 0.197689950466156\n13930 Training loss: 0.22335413098335266\n13940 Training loss: 0.19527389109134674\n13950 Training loss: 0.19140928983688354\n13960 Training loss: 0.18110185861587524\n13970 Training loss: 0.2161511480808258\n13980 Training loss: 0.13584452867507935\n13990 Training loss: 0.19937723875045776\n14000 Training loss: 0.2752973437309265\n14010 Training loss: 0.19463790953159332\n14020 Training loss: 0.22992965579032898\n14030 Training loss: 0.261318564414978\n14040 Training loss: 0.3113790452480316\n14050 Training loss: 0.23856033384799957\n14060 Training loss: 0.2611667513847351\n14070 Training loss: 0.2485227733850479\n14080 Training loss: 0.19389007985591888\n14090 Training loss: 0.2936190366744995\n14100 Training loss: 0.24089443683624268\n14110 Training loss: 0.2108636051416397\n14120 Training loss: 0.1840977519750595\n14130 Training loss: 0.23669975996017456\n14140 Training loss: 0.3331983685493469\n14150 Training loss: 0.20353104174137115\n14160 Training loss: 0.15950876474380493\n14170 Training loss: 0.23730115592479706\n14180 Training loss: 0.3005683124065399\n14190 Training loss: 0.1879369020462036\n14200 Training loss: 0.14957201480865479\n14210 Training loss: 0.2710883915424347\n14220 Training loss: 0.290828138589859\n14230 Training loss: 0.22995586693286896\n14240 Training loss: 0.3589620590209961\n14250 Training loss: 0.2796594202518463\n14260 Training loss: 0.457839697599411\n14270 Training loss: 0.23330479860305786\n14280 Training loss: 0.20454320311546326\n14290 Training loss: 0.2181909680366516\n14300 Training loss: 0.21497054398059845\n14310 Training loss: 0.29085657000541687\n14320 Training loss: 0.24665847420692444\n14330 Training loss: 0.25351327657699585\n14340 Training loss: 0.3906233608722687\n14350 Training loss: 0.2621039152145386\n14360 Training loss: 0.221028134226799\n14370 Training loss: 0.2653174102306366\n14380 Training loss: 0.22901985049247742\n14390 Training loss: 0.1493024080991745\n14400 Training loss: 0.2226889729499817\n14410 Training loss: 0.1687406599521637\n14420 Training loss: 0.2702413499355316\n14430 Training loss: 0.1886839121580124\n14440 Training loss: 0.29695791006088257\n14450 Training loss: 0.2995676100254059\n14460 Training loss: 0.2567231059074402\n14470 Training loss: 0.17789125442504883\n14480 Training loss: 0.20298990607261658\n14490 Training loss: 0.24675720930099487\n14500 Training loss: 0.25503644347190857\n14510 Training loss: 0.2527259886264801\n14520 Training loss: 0.23876304924488068\n14530 Training loss: 0.20184563100337982\n14540 Training loss: 0.27829888463020325\n14550 Training loss: 0.22535303235054016\n14560 Training loss: 0.3363136351108551\n14570 Training loss: 0.2558390498161316\n14580 Training loss: 0.2892131209373474\n14590 Training loss: 0.21184533834457397\n14600 Training loss: 0.18990115821361542\n14610 Training loss: 0.23343487083911896\n14620 Training loss: 0.13932910561561584\n14630 Training loss: 0.23771658539772034\n14640 Training loss: 0.19149380922317505\n14650 Training loss: 0.16890135407447815\n14660 Training loss: 0.1915854513645172\n14670 Training loss: 0.22712954878807068\n14680 Training loss: 0.20798617601394653\n14690 Training loss: 0.2791901230812073\n14700 Training loss: 0.18352897465229034\n14710 Training loss: 0.2695314586162567\n14720 Training loss: 0.225834459066391\n14730 Training loss: 0.177495539188385\n14740 Training loss: 0.17083168029785156\n14750 Training loss: 0.26883938908576965\n14760 Training loss: 0.1932685822248459\n14770 Training loss: 0.2294887900352478\n14780 Training loss: 0.2654569149017334\n14790 Training loss: 0.2603050470352173\n14800 Training loss: 0.2515069246292114\n14810 Training loss: 0.33352670073509216\n14820 Training loss: 0.23466143012046814\n14830 Training loss: 0.1315830796957016\n14840 Training loss: 0.1842963993549347\n14850 Training loss: 0.3234458565711975\n14860 Training loss: 0.19493736326694489\n14870 Training loss: 0.24323217570781708\n14880 Training loss: 0.2962309718132019\n14890 Training loss: 0.19044330716133118\n14900 Training loss: 0.22823305428028107\n14910 Training loss: 0.16936588287353516\n14920 Training loss: 0.2752642333507538\n14930 Training loss: 0.16530175507068634\n14940 Training loss: 0.19662673771381378\n14950 Training loss: 0.20230524241924286\n14960 Training loss: 0.22429704666137695\n14970 Training loss: 0.29953792691230774\n14980 Training loss: 0.28443610668182373\n14990 Training loss: 0.19700372219085693\n15000 Training loss: 0.13883531093597412\n15010 Training loss: 0.22850261628627777\n15020 Training loss: 0.2360198199748993\n15030 Training loss: 0.1762033849954605\n15040 Training loss: 0.21878580749034882\n15050 Training loss: 0.19420766830444336\n15060 Training loss: 0.2927364408969879\n15070 Training loss: 0.22930487990379333\n15080 Training loss: 0.29339364171028137\n15090 Training loss: 0.16719448566436768\n15100 Training loss: 0.15700653195381165\n15110 Training loss: 0.2616685628890991\n15120 Training loss: 0.36348167061805725\n15130 Training loss: 0.20669002830982208\n15140 Training loss: 0.24794374406337738\n15150 Training loss: 0.2902412712574005\n15160 Training loss: 0.25558578968048096\n15170 Training loss: 0.2064935714006424\n15180 Training loss: 0.20382654666900635\n15190 Training loss: 0.24467459321022034\n15200 Training loss: 0.2235407829284668\n15210 Training loss: 0.28220051527023315\n15220 Training loss: 0.197430819272995\n15230 Training loss: 0.30368903279304504\n15240 Training loss: 0.23150180280208588\n15250 Training loss: 0.20301347970962524\n15260 Training loss: 0.2484198808670044\n15270 Training loss: 0.2694310247898102\n15280 Training loss: 0.23136167228221893\n15290 Training loss: 0.21970252692699432\n15300 Training loss: 0.30802449584007263\n15310 Training loss: 0.10644038021564484\n15320 Training loss: 0.3308846950531006\n15330 Training loss: 0.2178054004907608\n15340 Training loss: 0.24417896568775177\n15350 Training loss: 0.22759601473808289\n15360 Training loss: 0.3223418891429901\n15370 Training loss: 0.2339540719985962\n15380 Training loss: 0.28745076060295105\n15390 Training loss: 0.298084557056427\n15400 Training loss: 0.17852169275283813\n15410 Training loss: 0.25896960496902466\n15420 Training loss: 0.23628127574920654\n15430 Training loss: 0.3626280725002289\n15440 Training loss: 0.2124691903591156\n15450 Training loss: 0.18343184888362885\n15460 Training loss: 0.22244639694690704\n15470 Training loss: 0.3211798071861267\n15480 Training loss: 0.29245102405548096\n15490 Training loss: 0.20585079491138458\n15500 Training loss: 0.16364392638206482\n15510 Training loss: 0.29332298040390015\n15520 Training loss: 0.3082224130630493\n15530 Training loss: 0.20818349719047546\n15540 Training loss: 0.14997048676013947\n15550 Training loss: 0.2522404193878174\n15560 Training loss: 0.24765804409980774\n15570 Training loss: 0.1923319399356842\n15580 Training loss: 0.1693575531244278\n15590 Training loss: 0.19331598281860352\n15600 Training loss: 0.24018479883670807\n15610 Training loss: 0.24542292952537537\n15620 Training loss: 0.26166197657585144\n15630 Training loss: 0.20897069573402405\n15640 Training loss: 0.19570232927799225\n15650 Training loss: 0.35166627168655396\n15660 Training loss: 0.26469698548316956\n15670 Training loss: 0.23958353698253632\n15680 Training loss: 0.22692517936229706\n15690 Training loss: 0.17127934098243713\n15700 Training loss: 0.29048624634742737\n15710 Training loss: 0.2513996362686157\n15720 Training loss: 0.17473188042640686\n15730 Training loss: 0.22791828215122223\n15740 Training loss: 0.2612457871437073\n15750 Training loss: 0.19355493783950806\n15760 Training loss: 0.19132280349731445\n15770 Training loss: 0.23381231725215912\n15780 Training loss: 0.2178545594215393\n15790 Training loss: 0.2104194611310959\n15800 Training loss: 0.19680853188037872\n15810 Training loss: 0.24143607914447784\n15820 Training loss: 0.19370949268341064\n15830 Training loss: 0.18746419250965118\n15840 Training loss: 0.19474004209041595\n15850 Training loss: 0.19540174305438995\n15860 Training loss: 0.2587283253669739\n15870 Training loss: 0.25467750430107117\n15880 Training loss: 0.19235992431640625\n15890 Training loss: 0.21266528964042664\n15900 Training loss: 0.284737765789032\n15910 Training loss: 0.19801771640777588\n15920 Training loss: 0.20483869314193726\n15930 Training loss: 0.19372186064720154\n15940 Training loss: 0.182535782456398\n15950 Training loss: 0.2685392200946808\n15960 Training loss: 0.18754425644874573\n15970 Training loss: 0.2684274911880493\n15980 Training loss: 0.19317294657230377\n15990 Training loss: 0.2829832136631012\n16000 Training loss: 0.3244558274745941\n16010 Training loss: 0.21723058819770813\n16020 Training loss: 0.41246455907821655\n16030 Training loss: 0.19506460428237915\n16040 Training loss: 0.278626948595047\n16050 Training loss: 0.2510281801223755\n16060 Training loss: 0.2781482934951782\n16070 Training loss: 0.19295616447925568\n16080 Training loss: 0.26765525341033936\n16090 Training loss: 0.20244711637496948\n16100 Training loss: 0.20406237244606018\n16110 Training loss: 0.3047372102737427\n16120 Training loss: 0.20983253419399261\n16130 Training loss: 0.20240114629268646\n16140 Training loss: 0.16818946599960327\n16150 Training loss: 0.17008286714553833\n16160 Training loss: 0.36071163415908813\n16170 Training loss: 0.2284925878047943\n16180 Training loss: 0.23774884641170502\n16190 Training loss: 0.20145973563194275\n16200 Training loss: 0.20124828815460205\n16210 Training loss: 0.21707011759281158\n16220 Training loss: 0.2301495373249054\n16230 Training loss: 0.2815570533275604\n16240 Training loss: 0.16566619277000427\n16250 Training loss: 0.21646495163440704\n16260 Training loss: 0.3359656035900116\n16270 Training loss: 0.19937950372695923\n16280 Training loss: 0.16077245771884918\n16290 Training loss: 0.27375367283821106\n16300 Training loss: 0.2105155885219574\n16310 Training loss: 0.27516913414001465\n16320 Training loss: 0.20116211473941803\n16330 Training loss: 0.2266220897436142\n16340 Training loss: 0.29025715589523315\n16350 Training loss: 0.14078526198863983\n16360 Training loss: 0.2918347418308258\n16370 Training loss: 0.2124025970697403\n16380 Training loss: 0.24051505327224731\n16390 Training loss: 0.28319281339645386\n16400 Training loss: 0.1806987226009369\n16410 Training loss: 0.22908249497413635\n16420 Training loss: 0.30434882640838623\n16430 Training loss: 0.18480423092842102\n16440 Training loss: 0.18991966545581818\n16450 Training loss: 0.17271234095096588\n16460 Training loss: 0.25308167934417725\n16470 Training loss: 0.19359779357910156\n16480 Training loss: 0.24045877158641815\n16490 Training loss: 0.20356301963329315\n16500 Training loss: 0.22898347675800323\n16510 Training loss: 0.22626131772994995\n16520 Training loss: 0.26772966980934143\n16530 Training loss: 0.21076148748397827\n16540 Training loss: 0.2058011293411255\n16550 Training loss: 0.21267174184322357\n16560 Training loss: 0.20781952142715454\n16570 Training loss: 0.25326403975486755\n16580 Training loss: 0.18003703653812408\n16590 Training loss: 0.265142560005188\n16600 Training loss: 0.30476659536361694\n16610 Training loss: 0.23104192316532135\n16620 Training loss: 0.20816980302333832\n16630 Training loss: 0.21422845125198364\n16640 Training loss: 0.19389112293720245\n16650 Training loss: 0.2605260908603668\n16660 Training loss: 0.16501958668231964\n16670 Training loss: 0.3378439247608185\n16680 Training loss: 0.22524291276931763\n16690 Training loss: 0.20465798676013947\n16700 Training loss: 0.26778480410575867\n16710 Training loss: 0.20544567704200745\n16720 Training loss: 0.2045181840658188\n16730 Training loss: 0.2678283751010895\n16740 Training loss: 0.16106823086738586\n16750 Training loss: 0.19096487760543823\n16760 Training loss: 0.26427963376045227\n16770 Training loss: 0.19792579114437103\n16780 Training loss: 0.2597152292728424\n16790 Training loss: 0.3350072503089905\n16800 Training loss: 0.16069887578487396\n16810 Training loss: 0.22159987688064575\n16820 Training loss: 0.23538021743297577\n16830 Training loss: 0.23394037783145905\n16840 Training loss: 0.2623237371444702\n16850 Training loss: 0.22918996214866638\n16860 Training loss: 0.2298813760280609\n16870 Training loss: 0.16886596381664276\n16880 Training loss: 0.3592190444469452\n16890 Training loss: 0.36218464374542236\n16900 Training loss: 0.2623830735683441\n16910 Training loss: 0.202730193734169\n16920 Training loss: 0.2441367357969284\n16930 Training loss: 0.24444936215877533\n16940 Training loss: 0.1951904147863388\n16950 Training loss: 0.21288441121578217\n16960 Training loss: 0.17770452797412872\n16970 Training loss: 0.2583191394805908\n16980 Training loss: 0.2524409294128418\n16990 Training loss: 0.2248254269361496\n17000 Training loss: 0.19262506067752838\n17010 Training loss: 0.21296289563179016\n17020 Training loss: 0.28769537806510925\n17030 Training loss: 0.252302348613739\n17040 Training loss: 0.2607991695404053\n17050 Training loss: 0.18823888897895813\n17060 Training loss: 0.18527542054653168\n17070 Training loss: 0.16725370287895203\n17080 Training loss: 0.19925187528133392\n17090 Training loss: 0.22818100452423096\n17100 Training loss: 0.184836745262146\n17110 Training loss: 0.20051947236061096\n17120 Training loss: 0.26204386353492737\n17130 Training loss: 0.24995703995227814\n17140 Training loss: 0.17083072662353516\n17150 Training loss: 0.23637796938419342\n17160 Training loss: 0.2441694438457489\n17170 Training loss: 0.14300695061683655\n17180 Training loss: 0.24132385849952698\n17190 Training loss: 0.2600366175174713\n17200 Training loss: 0.24540436267852783\n17210 Training loss: 0.2405223399400711\n17220 Training loss: 0.12143273651599884\n17230 Training loss: 0.23350025713443756\n17240 Training loss: 0.1923312246799469\n17250 Training loss: 0.216041699051857\n17260 Training loss: 0.2739654779434204\n17270 Training loss: 0.21622778475284576\n17280 Training loss: 0.22383585572242737\n17290 Training loss: 0.35921794176101685\n17300 Training loss: 0.20254169404506683\n17310 Training loss: 0.21152980625629425\n17320 Training loss: 0.2369733303785324\n17330 Training loss: 0.19489848613739014\n17340 Training loss: 0.17769069969654083\n17350 Training loss: 0.2618747055530548\n17360 Training loss: 0.1643173098564148\n17370 Training loss: 0.22915957868099213\n17380 Training loss: 0.2460373342037201\n17390 Training loss: 0.2511571943759918\n17400 Training loss: 0.1899544596672058\n17410 Training loss: 0.13895367085933685\n17420 Training loss: 0.28460049629211426\n17430 Training loss: 0.21215389668941498\n17440 Training loss: 0.1944841891527176\n17450 Training loss: 0.2249256670475006\n17460 Training loss: 0.2891347408294678\n17470 Training loss: 0.19109901785850525\n17480 Training loss: 0.23009265959262848\n17490 Training loss: 0.1919461041688919\n17500 Training loss: 0.18403545022010803\n17510 Training loss: 0.2377825379371643\n17520 Training loss: 0.2524280846118927\n17530 Training loss: 0.1731027066707611\n17540 Training loss: 0.20342221856117249\n17550 Training loss: 0.26169267296791077\n17560 Training loss: 0.18991291522979736\n17570 Training loss: 0.2158786654472351\n17580 Training loss: 0.24069495499134064\n17590 Training loss: 0.17989163100719452\n17600 Training loss: 0.15951351821422577\n17610 Training loss: 0.2592834532260895\n17620 Training loss: 0.22952516376972198\n17630 Training loss: 0.20921184122562408\n17640 Training loss: 0.20238324999809265\n17650 Training loss: 0.22923582792282104\n17660 Training loss: 0.19078901410102844\n17670 Training loss: 0.23539330065250397\n17680 Training loss: 0.27721086144447327\n17690 Training loss: 0.19255363941192627\n17700 Training loss: 0.1780799925327301\n17710 Training loss: 0.30796632170677185\n17720 Training loss: 0.2531360685825348\n17730 Training loss: 0.16197246313095093\n17740 Training loss: 0.1733892560005188\n17750 Training loss: 0.23395919799804688\n17760 Training loss: 0.24728071689605713\n17770 Training loss: 0.2346695065498352\n17780 Training loss: 0.20606161653995514\n17790 Training loss: 0.22656993567943573\n17800 Training loss: 0.22275659441947937\n17810 Training loss: 0.1843729019165039\n17820 Training loss: 0.127701997756958\n17830 Training loss: 0.3216648995876312\n17840 Training loss: 0.23403999209403992\n17850 Training loss: 0.215083509683609\n17860 Training loss: 0.27634263038635254\n17870 Training loss: 0.2260921150445938\n17880 Training loss: 0.24494926631450653\n17890 Training loss: 0.2477513551712036\n17900 Training loss: 0.262849897146225\n17910 Training loss: 0.25077903270721436\n17920 Training loss: 0.26959943771362305\n17930 Training loss: 0.2272152304649353\n17940 Training loss: 0.21761201322078705\n17950 Training loss: 0.21198059618473053\n17960 Training loss: 0.213704913854599\n17970 Training loss: 0.16992950439453125\n17980 Training loss: 0.16965727508068085\n17990 Training loss: 0.2426588386297226\n18000 Training loss: 0.196843221783638\n18010 Training loss: 0.220835343003273\n18020 Training loss: 0.21748819947242737\n18030 Training loss: 0.1942826509475708\n18040 Training loss: 0.23207718133926392\n18050 Training loss: 0.23142997920513153\n18060 Training loss: 0.17102529108524323\n18070 Training loss: 0.14637403190135956\n18080 Training loss: 0.22979381680488586\n18090 Training loss: 0.27779072523117065\n18100 Training loss: 0.20675520598888397\n18110 Training loss: 0.2738599479198456\n18120 Training loss: 0.20308226346969604\n18130 Training loss: 0.17990785837173462\n18140 Training loss: 0.23637573421001434\n18150 Training loss: 0.22333548963069916\n18160 Training loss: 0.2939366102218628\n18170 Training loss: 0.1612645536661148\n18180 Training loss: 0.24210409820079803\n18190 Training loss: 0.2606738209724426\n18200 Training loss: 0.15724480152130127\n18210 Training loss: 0.21257086098194122\n18220 Training loss: 0.19274358451366425\n18230 Training loss: 0.2754824161529541\n18240 Training loss: 0.2504838705062866\n18250 Training loss: 0.19042538106441498\n18260 Training loss: 0.21165309846401215\n18270 Training loss: 0.27395883202552795\n18280 Training loss: 0.1641930788755417\n18290 Training loss: 0.3134905695915222\n18300 Training loss: 0.22172811627388\n18310 Training loss: 0.28978851437568665\n18320 Training loss: 0.14017583429813385\n18330 Training loss: 0.2906256318092346\n18340 Training loss: 0.13943275809288025\n18350 Training loss: 0.29569458961486816\n18360 Training loss: 0.20006634294986725\n18370 Training loss: 0.26240262389183044\n18380 Training loss: 0.2658558189868927\n18390 Training loss: 0.3412230610847473\n18400 Training loss: 0.147433340549469\n18410 Training loss: 0.1666836142539978\n18420 Training loss: 0.25433406233787537\n18430 Training loss: 0.20834940671920776\n18440 Training loss: 0.22469936311244965\n18450 Training loss: 0.1757262945175171\n18460 Training loss: 0.31012073159217834\n18470 Training loss: 0.20374244451522827\n18480 Training loss: 0.2588688135147095\n18490 Training loss: 0.19544294476509094\n18500 Training loss: 0.3365796208381653\n18510 Training loss: 0.24964775145053864\n18520 Training loss: 0.25218239426612854\n18530 Training loss: 0.2225250005722046\n18540 Training loss: 0.23369604349136353\n18550 Training loss: 0.2626775801181793\n18560 Training loss: 0.24709564447402954\n18570 Training loss: 0.19537808001041412\n18580 Training loss: 0.24465139210224152\n18590 Training loss: 0.2544499337673187\n18600 Training loss: 0.18702200055122375\n18610 Training loss: 0.21060071885585785\n18620 Training loss: 0.15409958362579346\n18630 Training loss: 0.21803179383277893\n18640 Training loss: 0.27161964774131775\n18650 Training loss: 0.24849218130111694\n18660 Training loss: 0.26851192116737366\n18670 Training loss: 0.1614745557308197\n18680 Training loss: 0.21957167983055115\n18690 Training loss: 0.2536942660808563\n18700 Training loss: 0.1866174191236496\n18710 Training loss: 0.2217625379562378\n18720 Training loss: 0.2065739780664444\n18730 Training loss: 0.21189287304878235\n18740 Training loss: 0.19597025215625763\n18750 Training loss: 0.22695785760879517\n18760 Training loss: 0.2292492687702179\n18770 Training loss: 0.23860320448875427\n18780 Training loss: 0.22173668444156647\n18790 Training loss: 0.19143953919410706\n18800 Training loss: 0.21555717289447784\n18810 Training loss: 0.2088552713394165\n18820 Training loss: 0.21923547983169556\n18830 Training loss: 0.1851794421672821\n18840 Training loss: 0.27496597170829773\n18850 Training loss: 0.21402707695960999\n18860 Training loss: 0.27881476283073425\n18870 Training loss: 0.31506016850471497\n18880 Training loss: 0.19126424193382263\n18890 Training loss: 0.2401942014694214\n18900 Training loss: 0.3200695514678955\n18910 Training loss: 0.27706560492515564\n18920 Training loss: 0.23352035880088806\n18930 Training loss: 0.19653435051441193\n18940 Training loss: 0.3114745318889618\n18950 Training loss: 0.214024618268013\n18960 Training loss: 0.2049168348312378\n18970 Training loss: 0.1570882946252823\n18980 Training loss: 0.2111130654811859\n18990 Training loss: 0.1813010722398758\n19000 Training loss: 0.26310020685195923\n19010 Training loss: 0.18541258573532104\n19020 Training loss: 0.1652699112892151\n19030 Training loss: 0.22724506258964539\n19040 Training loss: 0.195339635014534\n19050 Training loss: 0.12573495507240295\n19060 Training loss: 0.20129325985908508\n19070 Training loss: 0.1771630048751831\n19080 Training loss: 0.2331194430589676\n19090 Training loss: 0.285309761762619\n19100 Training loss: 0.19744375348091125\n19110 Training loss: 0.2512834966182709\n19120 Training loss: 0.2340448498725891\n19130 Training loss: 0.2032950073480606\n19140 Training loss: 0.18214523792266846\n19150 Training loss: 0.15980635583400726\n19160 Training loss: 0.23270118236541748\n19170 Training loss: 0.21856588125228882\n19180 Training loss: 0.19135768711566925\n19190 Training loss: 0.190315380692482\n19200 Training loss: 0.22785979509353638\n19210 Training loss: 0.1414201557636261\n19220 Training loss: 0.24092592298984528\n19230 Training loss: 0.20320621132850647\n19240 Training loss: 0.2380465567111969\n19250 Training loss: 0.2013034075498581\n19260 Training loss: 0.271770715713501\n19270 Training loss: 0.18052153289318085\n19280 Training loss: 0.15767794847488403\n19290 Training loss: 0.29838836193084717\n19300 Training loss: 0.29289230704307556\n19310 Training loss: 0.1759095937013626\n19320 Training loss: 0.15688517689704895\n19330 Training loss: 0.2189561277627945\n19340 Training loss: 0.20247437059879303\n19350 Training loss: 0.24326254427433014\n19360 Training loss: 0.1396084427833557\n19370 Training loss: 0.203119158744812\n19380 Training loss: 0.26271551847457886\n19390 Training loss: 0.17363348603248596\n19400 Training loss: 0.21548651158809662\n19410 Training loss: 0.21578644216060638\n19420 Training loss: 0.28759440779685974\n19430 Training loss: 0.22870010137557983\n19440 Training loss: 0.21702180802822113\n19450 Training loss: 0.21738359332084656\n19460 Training loss: 0.25880056619644165\n19470 Training loss: 0.18506860733032227\n19480 Training loss: 0.31045952439308167\n19490 Training loss: 0.2562383711338043\n19500 Training loss: 0.244690403342247\n19510 Training loss: 0.30697670578956604\n19520 Training loss: 0.23962782323360443\n19530 Training loss: 0.18820519745349884\n19540 Training loss: 0.2145916223526001\n19550 Training loss: 0.2663866877555847\n19560 Training loss: 0.25001612305641174\n19570 Training loss: 0.187452033162117\n19580 Training loss: 0.21831898391246796\n19590 Training loss: 0.22197061777114868\n19600 Training loss: 0.1744738221168518\n19610 Training loss: 0.21322861313819885\n19620 Training loss: 0.20994234085083008\n19630 Training loss: 0.24323353171348572\n19640 Training loss: 0.16181251406669617\n19650 Training loss: 0.28814828395843506\n19660 Training loss: 0.32655254006385803\n19670 Training loss: 0.20055848360061646\n19680 Training loss: 0.26276516914367676\n19690 Training loss: 0.24339555203914642\n19700 Training loss: 0.1404387652873993\n19710 Training loss: 0.4067765772342682\n19720 Training loss: 0.19736814498901367\n19730 Training loss: 0.21462063491344452\n19740 Training loss: 0.22589275240898132\n19750 Training loss: 0.27011844515800476\n19760 Training loss: 0.2276267409324646\n19770 Training loss: 0.2159966081380844\n19780 Training loss: 0.18491226434707642\n19790 Training loss: 0.21641525626182556\n19800 Training loss: 0.20228758454322815\n19810 Training loss: 0.2337675541639328\n19820 Training loss: 0.24998915195465088\n19830 Training loss: 0.2231784462928772\n19840 Training loss: 0.21849115192890167\n19850 Training loss: 0.2350299209356308\n19860 Training loss: 0.18790869414806366\n19870 Training loss: 0.19199787080287933\n19880 Training loss: 0.2375129759311676\n19890 Training loss: 0.26127174496650696\n19900 Training loss: 0.15797020494937897\n19910 Training loss: 0.16658101975917816\n19920 Training loss: 0.22505296766757965\n19930 Training loss: 0.24901621043682098\n19940 Training loss: 0.18306311964988708\n19950 Training loss: 0.19728387892246246\n19960 Training loss: 0.2865627110004425\n19970 Training loss: 0.23585733771324158\n19980 Training loss: 0.21239914000034332\n19990 Training loss: 0.1731031984090805\n20000 Training loss: 0.1656932383775711\n20010 Training loss: 0.176860049366951\n20020 Training loss: 0.263703316450119\n20030 Training loss: 0.32223156094551086\n20040 Training loss: 0.1577846109867096\n20050 Training loss: 0.23689860105514526\n20060 Training loss: 0.17987960577011108\n20070 Training loss: 0.3040428161621094\n20080 Training loss: 0.1841985136270523\n20090 Training loss: 0.2439059615135193\n20100 Training loss: 0.19617511332035065\n20110 Training loss: 0.23710604012012482\n20120 Training loss: 0.23445914685726166\n20130 Training loss: 0.22899070382118225\n20140 Training loss: 0.1769496202468872\n20150 Training loss: 0.16683106124401093\n20160 Training loss: 0.2056441307067871\n20170 Training loss: 0.37925076484680176\n20180 Training loss: 0.16916127502918243\n20190 Training loss: 0.24051538109779358\n20200 Training loss: 0.22334636747837067\n20210 Training loss: 0.2596627175807953\n20220 Training loss: 0.23467028141021729\n20230 Training loss: 0.2995484173297882\n20240 Training loss: 0.18635393679141998\n20250 Training loss: 0.22686263918876648\n20260 Training loss: 0.28032082319259644\n20270 Training loss: 0.3094845116138458\n20280 Training loss: 0.18829436600208282\n20290 Training loss: 0.18441355228424072\n20300 Training loss: 0.3070991635322571\n20310 Training loss: 0.3159278631210327\n20320 Training loss: 0.1594785898923874\n20330 Training loss: 0.2730609178543091\n20340 Training loss: 0.22540120780467987\n20350 Training loss: 0.20757636427879333\n20360 Training loss: 0.3152010142803192\n20370 Training loss: 0.20244483649730682\n20380 Training loss: 0.22107723355293274\n20390 Training loss: 0.1771283596754074\n20400 Training loss: 0.2809871435165405\n20410 Training loss: 0.20718379318714142\n20420 Training loss: 0.18104158341884613\n20430 Training loss: 0.24968701601028442\n20440 Training loss: 0.20417656004428864\n20450 Training loss: 0.21565128862857819\n20460 Training loss: 0.21837608516216278\n20470 Training loss: 0.1920745074748993\n20480 Training loss: 0.20501184463500977\n20490 Training loss: 0.2362353354692459\n20500 Training loss: 0.21688739955425262\n20510 Training loss: 0.274846613407135\n20520 Training loss: 0.24415215849876404\n20530 Training loss: 0.2707204818725586\n20540 Training loss: 0.21014824509620667\n20550 Training loss: 0.16357086598873138\n20560 Training loss: 0.2806498408317566\n20570 Training loss: 0.16581371426582336\n20580 Training loss: 0.19425106048583984\n20590 Training loss: 0.15103550255298615\n20600 Training loss: 0.2221602201461792\n20610 Training loss: 0.17725740373134613\n20620 Training loss: 0.2003519982099533\n20630 Training loss: 0.253207802772522\n20640 Training loss: 0.2636052668094635\n20650 Training loss: 0.296100378036499\n20660 Training loss: 0.20496425032615662\n20670 Training loss: 0.24767670035362244\n20680 Training loss: 0.20284439623355865\n20690 Training loss: 0.13284119963645935\n20700 Training loss: 0.23653750121593475\n20710 Training loss: 0.21317452192306519\n20720 Training loss: 0.16917331516742706\n20730 Training loss: 0.22550205886363983\n20740 Training loss: 0.2838737368583679\n20750 Training loss: 0.24016670882701874\n20760 Training loss: 0.18107645213603973\n20770 Training loss: 0.1775166392326355\n20780 Training loss: 0.24707108736038208\n20790 Training loss: 0.1921953707933426\n20800 Training loss: 0.22219224274158478\n20810 Training loss: 0.18315614759922028\n20820 Training loss: 0.17010587453842163\n20830 Training loss: 0.3151814341545105\n20840 Training loss: 0.15373660624027252\n20850 Training loss: 0.15794262290000916\n20860 Training loss: 0.23130567371845245\n20870 Training loss: 0.20728738605976105\n20880 Training loss: 0.2389409989118576\n20890 Training loss: 0.163001149892807\n20900 Training loss: 0.1553948074579239\n20910 Training loss: 0.24208873510360718\n20920 Training loss: 0.18231691420078278\n20930 Training loss: 0.32450154423713684\n20940 Training loss: 0.2095058262348175\n20950 Training loss: 0.16044920682907104\n20960 Training loss: 0.20362995564937592\n20970 Training loss: 0.22511830925941467\n20980 Training loss: 0.1983388364315033\n20990 Training loss: 0.22572451829910278\n21000 Training loss: 0.2163354754447937\n21010 Training loss: 0.21822689473628998\n21020 Training loss: 0.23148423433303833\n21030 Training loss: 0.20900583267211914\n21040 Training loss: 0.2620851993560791\n21050 Training loss: 0.1955500990152359\n21060 Training loss: 0.226943239569664\n21070 Training loss: 0.2942012846469879\n21080 Training loss: 0.2541426718235016\n21090 Training loss: 0.23822756111621857\n21100 Training loss: 0.16301344335079193\n21110 Training loss: 0.2103777378797531\n21120 Training loss: 0.1961662918329239\n21130 Training loss: 0.29213494062423706\n21140 Training loss: 0.18985100090503693\n21150 Training loss: 0.20375151932239532\n21160 Training loss: 0.23184481263160706\n21170 Training loss: 0.2381267547607422\n21180 Training loss: 0.24501042068004608\n21190 Training loss: 0.2624693214893341\n21200 Training loss: 0.12837427854537964\n21210 Training loss: 0.19525152444839478\n21220 Training loss: 0.2791963517665863\n21230 Training loss: 0.21107210218906403\n21240 Training loss: 0.22014769911766052\n21250 Training loss: 0.17491108179092407\n21260 Training loss: 0.22835466265678406\n21270 Training loss: 0.2584267258644104\n21280 Training loss: 0.2152436375617981\n21290 Training loss: 0.14779317378997803\n21300 Training loss: 0.2459562122821808\n21310 Training loss: 0.24330255389213562\n21320 Training loss: 0.17373032867908478\n21330 Training loss: 0.23746462166309357\n21340 Training loss: 0.20245879888534546\n21350 Training loss: 0.21447256207466125\n21360 Training loss: 0.2212502360343933\n21370 Training loss: 0.19765663146972656\n21380 Training loss: 0.22401577234268188\n21390 Training loss: 0.1632402092218399\n21400 Training loss: 0.35183608531951904\n21410 Training loss: 0.20712336897850037\n21420 Training loss: 0.17008081078529358\n21430 Training loss: 0.21381773054599762\n21440 Training loss: 0.19974270462989807\n21450 Training loss: 0.2050318866968155\n21460 Training loss: 0.14589568972587585\n21470 Training loss: 0.18699924647808075\n21480 Training loss: 0.20005211234092712\n21490 Training loss: 0.2404896318912506\n21500 Training loss: 0.2470877468585968\n21510 Training loss: 0.2080928236246109\n21520 Training loss: 0.1838916391134262\n21530 Training loss: 0.21975074708461761\n21540 Training loss: 0.1804768443107605\n21550 Training loss: 0.16656970977783203\n21560 Training loss: 0.1472911834716797\n21570 Training loss: 0.21715663373470306\n21580 Training loss: 0.20030049979686737\n21590 Training loss: 0.17405037581920624\n21600 Training loss: 0.2099372297525406\n21610 Training loss: 0.2465633898973465\n21620 Training loss: 0.18479305505752563\n21630 Training loss: 0.19456776976585388\n21640 Training loss: 0.1799117624759674\n21650 Training loss: 0.23817558586597443\n21660 Training loss: 0.11048370599746704\n21670 Training loss: 0.20651865005493164\n21680 Training loss: 0.13785168528556824\n21690 Training loss: 0.205012708902359\n21700 Training loss: 0.2635897696018219\n21710 Training loss: 0.2581367790699005\n21720 Training loss: 0.15169882774353027\n21730 Training loss: 0.2579340934753418\n21740 Training loss: 0.1736575961112976\n21750 Training loss: 0.22907263040542603\n21760 Training loss: 0.23795154690742493\n21770 Training loss: 0.178267702460289\n21780 Training loss: 0.1691298484802246\n21790 Training loss: 0.23085561394691467\n21800 Training loss: 0.18026159703731537\n21810 Training loss: 0.18674038350582123\n21820 Training loss: 0.18202216923236847\n21830 Training loss: 0.22449606657028198\n21840 Training loss: 0.25435715913772583\n21850 Training loss: 0.19748321175575256\n21860 Training loss: 0.235262930393219\n21870 Training loss: 0.1871797889471054\n21880 Training loss: 0.2917163372039795\n21890 Training loss: 0.20234453678131104\n21900 Training loss: 0.1703694462776184\n21910 Training loss: 0.28764808177948\n21920 Training loss: 0.2398729771375656\n21930 Training loss: 0.18287459015846252\n21940 Training loss: 0.29556772112846375\n21950 Training loss: 0.3049060106277466\n21960 Training loss: 0.2243119776248932\n21970 Training loss: 0.23607628047466278\n21980 Training loss: 0.2005198448896408\n21990 Training loss: 0.19084030389785767\n22000 Training loss: 0.22968068718910217\n22010 Training loss: 0.23729243874549866\n22020 Training loss: 0.2651478350162506\n22030 Training loss: 0.17402935028076172\n22040 Training loss: 0.2525554895401001\n22050 Training loss: 0.2617533206939697\n22060 Training loss: 0.1929115653038025\n22070 Training loss: 0.2609827518463135\n22080 Training loss: 0.16010482609272003\n22090 Training loss: 0.24301278591156006\n22100 Training loss: 0.25524505972862244\n22110 Training loss: 0.22247803211212158\n22120 Training loss: 0.18051064014434814\n22130 Training loss: 0.1842365860939026\n22140 Training loss: 0.2390429824590683\n22150 Training loss: 0.24077825248241425\n22160 Training loss: 0.1984419822692871\n22170 Training loss: 0.19823625683784485\n22180 Training loss: 0.21848414838314056\n22190 Training loss: 0.23723042011260986\n22200 Training loss: 0.17636552453041077\n22210 Training loss: 0.18320053815841675\n22220 Training loss: 0.19255800545215607\n22230 Training loss: 0.1867738515138626\n22240 Training loss: 0.26748600602149963\n22250 Training loss: 0.23943671584129333\n22260 Training loss: 0.1997874230146408\n22270 Training loss: 0.20210891962051392\n22280 Training loss: 0.371318519115448\n22290 Training loss: 0.25470176339149475\n22300 Training loss: 0.22764652967453003\n22310 Training loss: 0.2772092819213867\n22320 Training loss: 0.19046972692012787\n22330 Training loss: 0.21098002791404724\n22340 Training loss: 0.24727535247802734\n22350 Training loss: 0.18015290796756744\n22360 Training loss: 0.25585076212882996\n22370 Training loss: 0.19869355857372284\n22380 Training loss: 0.22613495588302612\n22390 Training loss: 0.26685526967048645\n22400 Training loss: 0.21797014772891998\n22410 Training loss: 0.2695055902004242\n22420 Training loss: 0.18566638231277466\n22430 Training loss: 0.19857139885425568\n22440 Training loss: 0.23453962802886963\n22450 Training loss: 0.18109048902988434\n22460 Training loss: 0.30485668778419495\n22470 Training loss: 0.1736113429069519\n22480 Training loss: 0.26477375626564026\n22490 Training loss: 0.2190777063369751\n22500 Training loss: 0.2406216412782669\n22510 Training loss: 0.2069748044013977\n22520 Training loss: 0.19714561104774475\n22530 Training loss: 0.24291424453258514\n22540 Training loss: 0.20200709998607635\n22550 Training loss: 0.2922841012477875\n22560 Training loss: 0.23684760928153992\n22570 Training loss: 0.22478139400482178\n22580 Training loss: 0.256137490272522\n22590 Training loss: 0.21053847670555115\n22600 Training loss: 0.1813681572675705\n22610 Training loss: 0.22867071628570557\n22620 Training loss: 0.20360928773880005\n22630 Training loss: 0.2477029263973236\n22640 Training loss: 0.2162671983242035\n22650 Training loss: 0.2569785416126251\n22660 Training loss: 0.1724838763475418\n22670 Training loss: 0.1870129555463791\n22680 Training loss: 0.1451905369758606\n22690 Training loss: 0.26342928409576416\n22700 Training loss: 0.16628940403461456\n22710 Training loss: 0.21608348190784454\n22720 Training loss: 0.28330838680267334\n22730 Training loss: 0.23754537105560303\n22740 Training loss: 0.20489905774593353\n22750 Training loss: 0.23673853278160095\n22760 Training loss: 0.22043900191783905\n22770 Training loss: 0.19633060693740845\n22780 Training loss: 0.20694498717784882\n22790 Training loss: 0.23252689838409424\n22800 Training loss: 0.17697028815746307\n22810 Training loss: 0.2379370480775833\n22820 Training loss: 0.2776758670806885\n22830 Training loss: 0.20528964698314667\n22840 Training loss: 0.26372525095939636\n22850 Training loss: 0.12145664542913437\n22860 Training loss: 0.19893069565296173\n22870 Training loss: 0.20190313458442688\n22880 Training loss: 0.2100912630558014\n22890 Training loss: 0.1521470993757248\n22900 Training loss: 0.21717685461044312\n22910 Training loss: 0.2673931121826172\n22920 Training loss: 0.18423572182655334\n22930 Training loss: 0.25000810623168945\n22940 Training loss: 0.26966172456741333\n22950 Training loss: 0.20969396829605103\n22960 Training loss: 0.2268589437007904\n22970 Training loss: 0.22574786841869354\n22980 Training loss: 0.1580238938331604\n22990 Training loss: 0.1630340963602066\n23000 Training loss: 0.20727947354316711\n23010 Training loss: 0.21276544034481049\n23020 Training loss: 0.272335946559906\n23030 Training loss: 0.22797366976737976\n23040 Training loss: 0.18166670203208923\n23050 Training loss: 0.23573042452335358\n23060 Training loss: 0.2598266005516052\n23070 Training loss: 0.1892354041337967\n23080 Training loss: 0.24244344234466553\n23090 Training loss: 0.18495117127895355\n23100 Training loss: 0.18328359723091125\n23110 Training loss: 0.31159910559654236\n","output_type":"stream"}]},{"cell_type":"code","source":"# Saving only the model weights\nmodel_weights_path = \"/kaggle/working/model_weights_qa.pth\"\ntorch.save(model.state_dict(), model_weights_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:28:51.055923Z","iopub.execute_input":"2024-05-04T14:28:51.056702Z","iopub.status.idle":"2024-05-04T14:28:55.029002Z","shell.execute_reply.started":"2024-05-04T14:28:51.056666Z","shell.execute_reply":"2024-05-04T14:28:55.027958Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# load validation and test dataset","metadata":{}},{"cell_type":"code","source":"def preprocess_data(input_file, output_file):\n    with open(input_file, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    processed_lines = []\n\n    for line in lines:\n        question, answers = line.strip().split('\\t')\n        # Ensure the question ends with a question mark\n        question = question.strip()\n        if not question.endswith('?'):\n            question += '?'\n\n        # Split answers if there are multiple answers separated by '|'\n        answers = answers.split('|')\n        for answer in answers:\n            processed_line = f\"{question}\\t{answer.strip()}\\n\"\n            processed_lines.append(processed_line)\n\n    # Write the processed lines to the output file\n    with open(output_file, 'w', encoding='utf-8') as file:\n        file.writelines(processed_lines)\n\n# Define your input and output file paths\ninput_file_path = '/kaggle/input/metaqa/qa_test.txt'\noutput_file_path = '/kaggle/working/qa_test_pp.txt'\n\n# Call the function with the file paths\npreprocess_data(input_file_path, output_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:49:03.036917Z","iopub.execute_input":"2024-05-04T14:49:03.037428Z","iopub.status.idle":"2024-05-04T14:49:03.084201Z","shell.execute_reply.started":"2024-05-04T14:49:03.037392Z","shell.execute_reply":"2024-05-04T14:49:03.083412Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def load_dataset(file_path):\n    inputs = []\n    targets = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            input_text, target_text = line.strip().split('\\t')\n            inputs.append(input_text)\n            targets.append(target_text)\n    return inputs, targets\nqa_val_inputs, qa_val_targets = load_dataset(\"/kaggle/working/qa_dev_pp.txt\")\nqa_test_inputs, qa_test_targets = load_dataset(\"/kaggle/working/qa_test_pp.txt\")\nprint(qa_val_inputs[:10], qa_val_targets[:10])\nprint(qa_test_inputs[:10], qa_test_targets[:10])","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:59:28.930217Z","iopub.execute_input":"2024-05-04T14:59:28.930586Z","iopub.status.idle":"2024-05-04T14:59:28.972190Z","shell.execute_reply.started":"2024-05-04T14:59:28.930555Z","shell.execute_reply":"2024-05-04T14:59:28.971307Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"['what movies did [Temuera Morrison] act in?', 'what movies did [Temuera Morrison] act in?', 'what movies did [Temuera Morrison] act in?', 'what movies did [Evelyn Venable] act in?', 'what movies did [Evelyn Venable] act in?', 'what movies did [Evelyn Venable] act in?', 'what does [Tom Cullen] act in?', 'what movies was [Shareeka Epps] an actor in?', 'what does [Peter Franzn] appear in?', 'what does [Peter Franzn] appear in?'] ['Once Were Warriors', 'Tracker', 'River Queen', 'Alice Adams', 'Death Takes a Holiday', 'The Little Colonel', 'Weekend', 'Half Nelson', 'Ambush', 'Dog Nail Clipper']\n['what does [Grgoire Colin] appear in?', '[Joe Thomas] appears in which movies?', '[Joe Thomas] appears in which movies?', 'what films did [Michelle Trachtenberg] star in?', 'what films did [Michelle Trachtenberg] star in?', 'what films did [Michelle Trachtenberg] star in?', 'what films did [Michelle Trachtenberg] star in?', 'what films did [Michelle Trachtenberg] star in?', 'what does [Helen Mack] star in?', 'what does [Helen Mack] star in?'] ['Before the Rain', 'The Inbetweeners Movie', 'The Inbetweeners 2', 'Inspector Gadget', 'Black Christmas', 'Ice Princess', 'Harriet the Spy', 'The Scribbler', 'The Son of Kong', 'Kiss and Make-Up']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# tokenise data","metadata":{}},{"cell_type":"code","source":"def tokenize_data(inputs, targets, tokenizer, max_input_length=128, max_target_length=50):\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize training, validation, and test sets\nqa_val_data = tokenize_data(qa_val_inputs, qa_val_targets, tokenizer)\nqa_test_data = tokenize_data(qa_test_inputs, qa_test_targets, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:10:42.021858Z","iopub.execute_input":"2024-05-04T15:10:42.022209Z","iopub.status.idle":"2024-05-04T15:10:48.946992Z","shell.execute_reply.started":"2024-05-04T15:10:42.022183Z","shell.execute_reply":"2024-05-04T15:10:48.946136Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass T5Dataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n# qa_train_dataset = T5Dataset(qa_train_data)\nqa_val_dataset = T5Dataset(qa_val_data)\nqa_test_dataset = T5Dataset(qa_test_data)\n\n# qa_train_dataloader = DataLoader(qa_train_dataset, batch_size=8, shuffle=True)\nqa_val_dataloader = DataLoader(qa_val_dataset, batch_size=8, shuffle=False)\nqa_test_dataloader = DataLoader(qa_test_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:15:09.719856Z","iopub.execute_input":"2024-05-04T15:15:09.720748Z","iopub.status.idle":"2024-05-04T15:15:09.727744Z","shell.execute_reply.started":"2024-05-04T15:15:09.720713Z","shell.execute_reply":"2024-05-04T15:15:09.726826Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef evaluate_model(model, dataloader):\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model.to(device)\n    model.eval()  # Make sure model is in eval mode\n    total_loss = 0\n    predictions, true_labels = [], []\n\n    with torch.no_grad():  # Disable gradient calculation\n        for batch in dataloader:\n            # Move batch to the same device as the model\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n\n            # Collect the loss and calculate the average\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            # Decode predictions\n            predicted_ids = torch.argmax(outputs.logits, dim=-1)\n#             predicted_tokens = [tokenizer.decode(ids) for ids in predicted_ids]\n            predicted_tokens = [tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n            predictions.extend(predicted_tokens)\n            true_labels.extend([tokenizer.decode(ids, skip_special_tokens=True) for ids in batch['labels']])\n\n    # Calculate average loss\n    average_loss = total_loss / len(dataloader)\n    return predictions, true_labels, average_loss\n\n# Evaluate on validation and test datasets\nprint(\"----- Validation -----\")\nqa_val_predictions, qa_val_labels, qa_val_loss = evaluate_model(model, qa_val_dataloader)\nprint(f\"Validation loss: {qa_val_loss}\")\nprint(\"----- Test -----\")\nqa_test_predictions, qa_test_labels, qa_test_loss = evaluate_model(model, qa_test_dataloader)\nprint(f\"Test loss: {qa_test_loss}\")\n\n# Here you can further calculate accuracy or other metrics based on predictions and true_labels\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:23:22.815759Z","iopub.execute_input":"2024-05-04T15:23:22.816465Z","iopub.status.idle":"2024-05-04T15:45:25.499542Z","shell.execute_reply.started":"2024-05-04T15:23:22.816431Z","shell.execute_reply":"2024-05-04T15:45:25.498602Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"----- Validation -----\nValidation loss: 0.20789015198650354\n----- Test -----\nTest loss: 0.20737239391877538\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_accuracy(predictions, true_labels):\n    correct_predictions = sum(1 for pred, true in zip(predictions, true_labels) if pred == true)\n    total_predictions = len(predictions)\n    accuracy = correct_predictions / total_predictions\n    print(correct_predictions)\n    return accuracy\n\n# Calculate accuracy\nqa_val_accuracy = calculate_accuracy(qa_val_predictions, qa_val_labels)\nqa_test_accuracy = calculate_accuracy(qa_test_predictions, qa_test_labels)\n\nprint(f\"Validation Loss: {qa_val_loss}, Validation Accuracy: {qa_val_accuracy}\")\nprint(f\"Test Loss: {qa_test_loss}, Test Accuracy: {qa_test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:52:33.801802Z","iopub.execute_input":"2024-05-04T15:52:33.802437Z","iopub.status.idle":"2024-05-04T15:52:33.817125Z","shell.execute_reply.started":"2024-05-04T15:52:33.802400Z","shell.execute_reply":"2024-05-04T15:52:33.816115Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"558\n623\nValidation Loss: 0.20789015198650354, Validation Accuracy: 0.029391624967079272\nTest Loss: 0.20737239391877538, Test Accuracy: 0.03220303938798718\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_hits_at_1(predictions, true_labels):\n    hits = 0\n    total = len(predictions)  # Total number of predictions\n\n    # Iterate over each prediction and corresponding true label\n    for pred, true in zip(predictions, true_labels):\n        # Increment the hits count if the top prediction matches the true label\n        if pred == true:\n            hits += 1\n\n    # Calculate the Hits@1 score\n    hits_at_1 = hits / total\n    return hits_at_1\n\n# Assuming val_predictions, val_labels, test_predictions, test_labels are defined\nval_hits_at_1 = calculate_hits_at_1(qa_val_predictions, qa_val_labels)\ntest_hits_at_1 = calculate_hits_at_1(qa_test_predictions, qa_test_labels)\n\nprint(f\"Validation Hits@1: {val_hits_at_1:.2f}\")\nprint(f\"Test Hits@1: {test_hits_at_1:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:56:23.238177Z","iopub.execute_input":"2024-05-04T15:56:23.238919Z","iopub.status.idle":"2024-05-04T15:56:23.252919Z","shell.execute_reply.started":"2024-05-04T15:56:23.238883Z","shell.execute_reply":"2024-05-04T15:56:23.252026Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Validation Hits@1: 0.03\nTest Hits@1: 0.03\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"[Mr. North] is a film written by this person?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n\n# Generate the output\nwith torch.no_grad():\n    outputs = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n\n# Decode the output\noutput_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(output_text)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T16:11:10.745819Z","iopub.execute_input":"2024-05-04T16:11:10.746242Z","iopub.status.idle":"2024-05-04T16:11:10.967122Z","shell.execute_reply.started":"2024-05-04T16:11:10.746206Z","shell.execute_reply":"2024-05-04T16:11:10.966216Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Claude Chabrol\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print some sample predictions and labels\n# for i in range(len(val_predictions)):\n#     if val\n#     print(f\"Prediction: {val_predictions[i]}, True Label: {val_labels[i]}\")\nfor pred, true in zip(qa_val_predictions, qa_val_labels):\n    if pred == true:\n        print(f\"Prediction: {pred}, True Label: {true}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T16:08:25.341852Z","iopub.execute_input":"2024-05-04T16:08:25.342519Z","iopub.status.idle":"2024-05-04T16:08:25.357414Z","shell.execute_reply.started":"2024-05-04T16:08:25.342483Z","shell.execute_reply":"2024-05-04T16:08:25.356412Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Prediction: John Wayne, True Label: John Wayne\nPrediction: John Wayne, True Label: John Wayne\nPrediction: John Wayne, True Label: John Wayne\nPrediction: John Wayne, True Label: John Wayne\nPrediction: John Wayne, True Label: John Wayne\nPrediction: John Wayne, True Label: John Wayne\nPrediction: John Wayne, True Label: John Wayne\nPrediction: John Wayne, True Label: John Wayne\nPrediction: John Wayne, True Label: John Wayne\nPrediction: John Wayne, True Label: John Wayne\nPrediction: John Wayne, True Label: John Wayne\nPrediction: John Huston, True Label: John Huston\nPrediction: Comedy, True Label: Comedy\nPrediction: Drama, True Label: Drama\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Drama, True Label: Drama\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Drama, True Label: Drama\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Drama, True Label: Drama\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Drama, True Label: Drama\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Drama, True Label: Drama\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Drama, True Label: Drama\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Drama, True Label: Drama\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: Comedy, True Label: Comedy\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: good, True Label: good\nPrediction: famous, True Label: famous\nPrediction: famous, True Label: famous\nPrediction: famous, True Label: famous\nPrediction: famous, True Label: famous\nPrediction: famous, True Label: famous\nPrediction: famous, True Label: famous\nPrediction: famous, True Label: famous\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: French, True Label: French\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: bd-r, True Label: bd-r\nPrediction: John Huston, True Label: John Huston\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: 2009, True Label: 2009\nPrediction: The Last of the Mohicans, True Label: The Last of the Mohicans\n","output_type":"stream"}]}]}